Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language Processing , pages 4582–4597
August 1–6, 2021. ©2021 Association for Computational Linguistics4582Preﬁx-Tuning: Optimizing Continuous Prompts for Generation
Xiang Lisa Li
Stanford University
xlisali@stanford.eduPercy Liang
Stanford University
pliang@cs.stanford.edu
Abstract
Fine-tuning is the de facto way of leveraging
large pretrained language models for down-
stream tasks. However, ﬁne-tuning modiﬁes
all the language model parameters and there-
fore necessitates storing a full copy for each
task. In this paper, we propose preﬁx-tuning, a
lightweight alternative to ﬁne-tuning for natu-
ral language generation tasks, which keeps lan-
guage model parameters frozen and instead op-
timizes a sequence of continuous task-speciﬁc
vectors, which we call the preﬁx . Preﬁx-tuning
draws inspiration from prompting for language
models, allowing subsequent tokens to attend
to this preﬁx as if it were “virtual tokens”.
We apply preﬁx-tuning to GPT-2 for table-to-
text generation and to BART for summariza-
tion. We show that by modifying only 0.1% of
the parameters, preﬁx-tuning obtains compara-
ble performance in the full data setting, outper-
forms ﬁne-tuning in low-data settings, and ex-
trapolates better to examples with topics that
are unseen during training.
1 Introduction
Fine-tuning is the prevalent paradigm for using
large pretrained language models (LMs) (Radford
et al., 2019; Devlin et al., 2019) to perform down-
stream tasks (e.g., summarization), but it requires
updating and storing all the parameters of the LM.
Consequently, to build and deploy NLP systems
that rely on large pretrained LMs, one currently
needs to store a modiﬁed copy of all the LM pa-
rameters for each task. This can be prohibitively
expensive given the size of current LMs; for exam-
ple, GPT-2 has 774M parameters (Radford et al.,
2019) and GPT-3 has 175B parameters (Brown
et al., 2020).
A natural approach to this problem is lightweight
ﬁne-tuning , which freezes most of the pretrained
parameters and only tunes a smaller set of param-
eters. For example, adapter-tuning (Rebufﬁ et al.,
Figure 1: Fine-tuning (top) updates all LM param-
eters (the red Transformer box) and requires storing
a full model copy for each task. We propose preﬁx-
tuning (bottom), which freezes the LM parameters and
only optimizes the preﬁx (the red preﬁx blocks). Con-
sequently, we only need to store the preﬁx for each
task, making preﬁx-tuning modular and space-efﬁcient.
Note that each vertical block denote transformer activa-
tions at one time step.
2017; Houlsby et al., 2019) inserts additional task-
speciﬁc layers between the layers of pretrained
language models. Adapter-tuning has promising
performance on natural language understanding
and generation benchmarks, attaining comparable
performance with ﬁne-tuning while adding only
around 2–4% task-speciﬁc parameters (Houlsby
et al., 2019; Lin et al., 2020).
At the limit, GPT-3 (Brown et al., 2020) can
be deployed using in-context learning, which is
a form of prompting , without modifying any LM
parameters. In in-context learning, Brown et al.
(2020) prepend a natural language task instruction
(e.g., TL;DR for summarization) and a few exam-
ples to the task input, and then generate the task
output from the LM. However, since Transformers
can only condition on a bounded-length context
(e.g., 2048 tokens for GPT-3), in-context learning
is restricted to very small training sets.

4583In this paper, we propose preﬁx-tuning , a
lightweight alternative to ﬁne-tuning for natural lan-
guage generation (NLG) tasks, inspired by prompt-
ing. Consider the task of generating a textual de-
scription of a data table, as shown in Figure 1,
where the task input is a linearized table (e.g.,
“name: Starbucksjtype: coffee shop”) and the out-
put is a textual description (e.g., “Starbucks serves
coffee.”). Preﬁx-tuning prepends a sequence of
continuous task-speciﬁc vectors to the input, which
we call a preﬁx , depicted by red blocks in Figure 1
(bottom). To generate each token, the LM can at-
tend to the preﬁx as if it were a sequence of “virtual
tokens”, but unlike prompting, the preﬁx consists
entirely of free parameters which do not correspond
to real tokens. In contrast to ﬁne-tuning in Figure 1
(top), which updates all LM parameters and thus
requires storing a tuned copy of the model for each
task, preﬁx-tuning only optimizes the preﬁx. Con-
sequently, we only need to store one copy of the
large LM and a learned task-speciﬁc preﬁx, yield-
ing a very small overhead for each additional task
(e.g., 250K parameters for table-to-text).
In contrast to full ﬁne-tuning, preﬁx-tuning is
also modular: we train an upstream preﬁx which
steers an unmodiﬁed LM, and therefore, a single
LM can support many tasks at once. In the con-
text of personalization where the tasks correspond
to users (Shokri and Shmatikov, 2015; McMahan
et al., 2016), we would have a separate preﬁx for
each user trained only on that user’s data, thereby
avoiding data cross-contamination. Moreover, the
preﬁx-based architecture enables us to even pro-
cess examples from multiple users/tasks in a single
batch, something that is not possible with other
lightweight ﬁne-tuning approaches like adapter-
tuning.
We evaluate preﬁx-tuning on table-to-text gen-
eration using GPT-2 and abstractive summariza-
tion using BART. In terms of storage, preﬁx-tuning
stores 1000x fewer parameters than full ﬁne-tuning.
In terms of performance when trained on full
datasets, preﬁx-tuning and ﬁne-tuning are compara-
ble for table-to-text ( x6.1), while preﬁx-tuning suf-
fers a small degradation for summarization ( x6.2).
In low-data settings, preﬁx-tuning outperforms ﬁne-
tuning on both tasks ( x6.3). Preﬁx-tuning also ex-
trapolates better to tables (for table-to-text) and arti-
cles (for summarization) with unseen topics ( x6.4).2 Related Work
Fine-tuning for natural language generation.
Current state-of-the-art systems for natural lan-
guage generation (NLG) are based on ﬁne-tuning
pretrained LMs. For table-to-text generation, Kale
(2020) ﬁne-tunes a sequence-to-sequence model
(T5; Raffel et al., 2020). For extractive and abstrac-
tive summarization, researchers ﬁne-tune masked
language models (e.g., BERT; Devlin et al., 2019)
and encode-decoder models (e.g., BART; Lewis
et al., 2020), respectively (Zhong et al., 2020; Liu
and Lapata, 2019; Raffel et al., 2020). For other
conditional NLG tasks such as machine transla-
tion and dialogue generation, ﬁne-tuning is also the
prevalent paradigm (Zhang et al., 2020c; Stickland
et al., 2020; Zhu et al., 2020; Liu et al., 2020). In
this paper, we focus on table-to-text using GPT-2
and summarization using BART, but preﬁx-tuning
in principle can be applied to other generation tasks
and pretrained models, such as masked LMs.
Lightweight ﬁne-tuning. Preﬁx-tuning falls
under the broad class of lightweight ﬁne-tuning
methods, which freeze most of the pretrained
parameters and only tune a smaller set of param-
eters. The key question is how to augment the LM
architecture and decide which subset of pretrained
parameters to tune. One line of research learns a
task-speciﬁc parameter mask (Zhao et al., 2020;
Radiya-Dixit and Wang, 2020). Another line
of research inserts new modules with trainable
parameters. For example, Zhang et al. (2020a)
trains a “side” network that is fused with the
pretrained model via summation; adapter-tuning
inserts task-speciﬁc layers (adapters) between each
layer of the pretrained LM (Houlsby et al., 2019;
Lin et al., 2020; Rebufﬁ et al., 2017; Pfeiffer et al.,
2020). Compared to this line of work, which tunes
around 3:6%of the LM parameters, our method
obtains a further 30x reduction in task-speciﬁc
parameters, tuning only 0.1% while maintaining
comparable performance on table-to-text tasks.
Prompting. Prompting is a way of leveraging a
pretrained LM by prepending instructions and a
few examples to the task input and generating the
task output from the LM. For autoregressive LMs,
the most successful form of prompting is GPT-3’s
in-context learning (Brown et al., 2020), which
uses manually designed prompts to adapt its gen-
eration for different tasks in few-shot settings. For
masked LMs like BERT and RoBERTa (Liu et al.,

45842019), prompt engineering has been explored for
natural language understanding tasks (Jiang et al.,
2020; Schick and Sch ¨utze, 2020). For example,
AutoPrompt (Shin et al., 2020) searches for a se-
quence of discrete trigger words and concatenates
it with each input to elicit sentiment or factual
knowledge from BERT and RoBERTa. In contrast
with AutoPrompt, our method optimizes contin-
uous preﬁxes, which are more expressive ( x7.2);
moreover, we focus on language generation tasks.
Continuous vectors have been used to steer LMs;
for example, Subramani et al. (2020) showed that a
pretrained LSTM language model can reconstruct
arbitrary sentences by optimizing a continuous vec-
tor for each sentence, making the vector input-
speciﬁc . In contrast, preﬁx-tuning optimizes a task-
speciﬁc preﬁx that applies to all instances of that
task. As a result, unlike the previous work whose
application is limited to sentence reconstruction,
preﬁx-tuning can be applied to NLG tasks.
Controllable generation. Controllable genera-
tion aims to steer a pretrained language model
to match a sentence-level attribute (e.g., positive
sentiment or sports). Such control can happen at
training time: Keskar et al. (2019) pretrains the
language model (CTRL) to condition on metadata
such as keywords or URLs. The control can also
happen at decoding time, by weighted decoding
(GeDi, Krause et al., 2020) or iteratively updat-
ing the past activations (PPLM, Dathathri et al.,
2020). However, there is no straightforward way
to apply these controllable generation techniques
to enforce ﬁne-grained control over generated con-
tents, as demanded by tasks like table-to-text and
summarization.
P*-tuning. Preﬁx tuning is an instance of a new
class of methods that has emerged, which we call
p*-tuning (since the other prominent instances, p-
tuning and prompt-tuning, also start with p), all
based on the idea of optimizing a continuous preﬁx
or prompt. Concurrent with our work, Qin and Eis-
ner (2021) learn mixtures of soft ﬁll-in-the-blank
prompts to elicit knowledge from LMs such as
BERT and BART. Hambardzumyan et al. (2021)
learns task-speciﬁc embeddings that adapts BERT
for sentiment classiﬁcation. Both works show that
tuning soft prompts outperforms previous work,
which optimizes over discrete prompts. P-tuning
(Liu et al., 2021) shows that jointly updating the
prompt embeddings and LM parameters improvesGPT-2’s performance on natural language under-
standing tasks, in both few-shot and full data set-
tings. In a followup work, Prompt-tuning (Lester
et al., 2021) simpliﬁes our approach and applies
it to T5 (Raffel et al., 2020), demonstrating that
the performance gap between ﬁne-tuning and p*-
tuning vanishes as the model size grows.
3 Problem Statement
Consider a conditional generation task where the
inputxis a context and the output yis a sequence
of tokens. We focus on two tasks, shown in Fig-
ure 2 (right): In table-to-text, xcorresponds to a lin-
earized data table and yis a textual description; in
summarization, xis an article and yis a summary.
3.1 Autoregressive LM
Assume we have an autoregressive neural language
modelp(yjx)parametrized by (e.g., GPT-2;
Radford et al., 2019). As shown in Figure 2 (top),
letz= [x;y]be the concatenation of xandy;
letXidxdenote the sequence of indices that corre-
sponds tox, and Yidxdenote the same for y.
The activation vector at time step iishi2Rd,
wherehi= [h(1)
i;;h(n)
i]is a concatenation of
all activation layers at this time step, and h(j)
iis the
activation vector of the j-th layer at time step i.1
An autoregressive neural LM computes hias a
function of ziand the past activations in its left
context, as follows:
hi=LM(zi;h<i), (1)
where the last layer of hiis used to compute the
distribution for the next token: p(zi+1jhi) =
softmax(Wh(n)
i)andWis a matrix that maps
h(n)
ito logits over the vocabulary.
3.2 Encoder-Decoder Architecture
We can also use an encoder-decoder architecture
(e.g., BART; Lewis et al., 2020) to model p(yjx),
wherexis encoded by the bidirectional encoder,
and the decoder predicts yautoregressively (condi-
tioned on the encoded xand its left context). We
use the same indexing and activation notation, as
shown in Figure 2 (bottom): each hifori2Xidx
is computed by the a bidirectional encoder; each
hifori2Yidxis computed by an autoregressive
decoder using the same equation (1).
1In GPT-2, h(n)
iconsists of a key-value pair, and the di-
mension of each key and value is 1024 .

4585
Figure 2: An annotated example of preﬁx-tuning using an autoregressive LM (top) and an encoder-decoder model
(bottom). The preﬁx activations 8i2Pidx;hiare drawn from a trainable matrix P. The remaining activations are
computed by the Transformer.
3.3 Fine-tuning
In the full ﬁne-tuning framework, we initialize with
the pretrained parameters . Herepis a train-
able language model distribution and we perform
gradient updates on the following log-likelihood
objective:
max
logp(yjx) = max
X
i2Yidxlogp(zijh<i).
(2)
4 Preﬁx-Tuning
We propose preﬁx-tuning as an alternative to full
ﬁne-tuning for conditional generation tasks. We
ﬁrst provide intuition in x4.1 before deﬁning our
method formally in x4.2.
4.1 Intuition
Prompting has demonstrated that conditioning on a
proper context can steer the LM without changing
its parameters. For example, if we want the LM
to generate a word (e.g., Obama), we can prepend
its common collocations as context (e.g., Barack),
and the LM will assign much higher probability to
the desired word. Extending this intuition beyond
generating a single word or sentence, we want to
ﬁnd a context that steers the LM to solve an NLG
task. Intuitively, the context could inﬂuence the
encoding of the task input xby guiding what to ex-
tract fromx, and it could inﬂuence the generation
of the task output yby steering the next token distri-
bution. However, it’s non-obvious whether such a
context exists. Using natural language task instruc-
tions (e.g., “summarize the following table in one
sentence”) for the context might guide a human tosolve the task, but this fails for moderately-sized
pretrained LMs.2Optimizing over the discrete in-
structions might help, but discrete optimization is
computationally challenging.
Instead of optimizing over discrete tokens, we
can optimize the instruction as continuous word em-
beddings, whose effects will be propagated upward
to all Transformer activation layers and rightward
to subsequent tokens. This is strictly more expres-
sive than a discrete prompt which is constrained to
the embeddings of real words. Preﬁx-tuning goes
one step further in increasing expressivity by op-
timizing the activations of all the layers, not just
the embedding layer. As another beneﬁt, preﬁx-
tuning can directly modify representations deeper
in the network, therefore, avoiding long computa-
tion paths across the depth of the network.
4.2 Method
Preﬁx-tuning prepends a preﬁx for an autoregres-
sive LM to obtain z= [PREFIX ;x;y], or prepends
preﬁxes for both encoder and decoder to obtain
z= [PREFIX ;x;PREFIX0;y], as shown in Figure 2.
Here, Pidxdenotes the sequence of preﬁx indices,
and we usejPidxjto denote the length of the preﬁx.
We follow the recurrence relation in equa-
tion (1), except that the activations of the preﬁx
indices are free parameters, given by a matrix P
(parametrized by ) of dimensionjPidxjdim(hi).
hi=(
P[i;:]; ifi2Pidx,
LM(zi;h<i);otherwise.(3)
2In our preliminary experiments, GPT-2 and BART fail in
this setting; the only exception is GPT-3.

4586The training objective is the same as equation (2),
but the set of trainable parameters changes: the lan-
guage model parameters are ﬁxed and the preﬁx
parametersare the only trainable parameters.
Here, eachhiis a function of the trainable P.
Wheni2Pidx, this is clear because hicopies
directly from P. Wheni62Pidx,histill depends
onP, because the preﬁx activations are always
in the left context and will therefore affect any
activations to the right.
4.3 Parametrization of P
Empirically, directly updating the Pparameters
leads to unstable optimization and a slight drop
in performance.3So we reparametrize the matrix
P[i;:] = MLP(P0
[i;:])by a smaller matrix ( P0
)
composed with a large feedforward neural network
(MLP). Now, the trainable parameters include P0

and the parameters of MLP. Note that Pand
P0
has the same number of rows (i.e., the preﬁx
length), but different number of columns.4
Once training is complete, these reparametriza-
tion parameters can be dropped, and only the preﬁx
(P) needs to be saved.
5 Experimental Setup
5.1 Datasets and Metrics
We evaluate on three standard neural generation
datasets for the table-to-text task: E2E (Novikova
et al., 2017), WebNLG (Gardent et al., 2017), and
DART (Radev et al., 2020), as shown in Table 1.
The datasets are ordered by increasing complexity
and size. E2E only has 1domain (i.e. restaurant
reviews); WebNLG has 14domains, and DART
is open-domain, using open-domain tables from
Wikipedia. For evaluation, we report the metrics
using the ofﬁcial evaluation scripts (see details in
Appendix A.1).
For the summarization task, we use the XSUM
(Narayan et al., 2018) dataset, which is an abstrac-
tive summarization dataset on news articles. We
report ROUGE-1, ROUGE-2 and ROUGE-L.
5.2 Methods
For table-to-text generation, we compare preﬁx-
tuning with three other methods: full ﬁne-tuning
3We ﬁnd in preliminary experiments that directly optimiz-
ing the preﬁx is very sensitive to initialization.
4Phas dimensionsjPidxjdim(hi)while Phas
dimensionsjPidxjk. We choose k= 512 for table-to-text
and800for summarization. MLPmaps from ktodim(hi).#examples input length output length
E2E 50K 28.5 27.8
WebNLG 22K 49.6 30.7
DART 82K 38.8 27.3
XSUM 225K 473.3 28.1
Table 1: Datasets statistics. The input and output
length is the number of BPE tokens per example. For
the three table-to-text datasets, the input length is the
length of linearized tables (details in Appendix A.1).
(FT- FULL ), ﬁne-tuning only the top 2 layers ( FT-
TOP2), and adapter-tuning ( ADAPTER ).5We also
report the current state-of-the-art results on these
datasets: On E2E, Shen et al. (2019) uses a prag-
matically informed model without pretraining. On
WebNLG, Kale (2020) ﬁne-tunes T5-large. On
DART, no ofﬁcial models trained on this dataset
version are released.6For summarization, we com-
pare against ﬁne-tuning BART (Lewis et al., 2020).
5.3 Architectures and Hyperparameters
For table-to-text, we use GPT-2 MEDIUM and GPT-
2LARGE . For summarization, we use BART LARGE .
Our implementation is based on the Hugging Face
Transformers (Wolf et al., 2020).
At training time, we use the AdamW optimizer
(Loshchilov and Hutter, 2019) and a linear learn-
ing rate scheduler, as suggested by the Hugging
Face default setup. The hyperparameters we tune
include the number of epochs, batch size, learning
rate, and preﬁx length. Hyperparameter details are
in the appendix. The default setting is 10epochs,
batch size 5, learning rate 510 5and preﬁx length
10. The table-to-text models are trained on TITAN
Xp or GeForce GTX TITAN X machines. Preﬁx-
tuning takes 0:2hours per epoch to train on 22K
examples, whereas ﬁne-tuning takes around 0:3
hours per epoch. The summarization models are
trained on Tesla V100 machines, taking 1:25hours
per epoch on the XSUM dataset. For time efﬁ-
ciency , preﬁx-tuning is around 30% faster than
ﬁne-tuning. For GPU memory efﬁciency , preﬁx-
tuning with batchsize 1 takes 18% of the total GPU
memory, whereas ﬁne-tuning takes 50%.
At decoding time, for table-to-text, we use beam
search with beam size 5. For summarization, we
use beam size 6and length normalization 0:8. De-
coding takes 1:2seconds per sentence (without
5Same implementation as Lin et al. (2020).
6The ofﬁcial benchmark model is trained on v.1.0.0 while
the release dataset is v1.1.1.

4587batching) for table-to-text, and 2:6seconds per
batch (using a batch size of 10) for summarization.
6 Main Results
6.1 Table-to-text Generation
We ﬁnd that by updating only 0.1% task-speciﬁc pa-
rameters,7preﬁx-tuning is effective in table-to-text
generation, outperforming other lightweight base-
lines ( ADAPTER andFT- TOP2) even by updating
30x fewer parameters and achieving a comparable
performance with (full) ﬁne-tuning. This trend
holds for all datasets: E2E, WebNLG,8and DART.
If we match the number of parameters for preﬁx-
tuning and adapter-tuning to be 0.1%, Table 2
shows that preﬁx-tuning is signiﬁcantly better than
ADAPTER (0.1%), attaining 4:1BLEU improve-
ment per dataset on average. Even when we com-
pare with ﬁne-tuning (100%) and adapter-tuning
(3.0%), which update signiﬁcantly more parame-
ters than preﬁx-tuning, preﬁx-tuning still achieves
results comparable or better than those two systems.
This demonstrates that preﬁx-tuning is more Pareto
efﬁcient than adapter-tuning, signiﬁcantly reducing
parameters while improving generation quality.
Additionally, attaining good performance on
DART suggests that preﬁx-tuning can generalize
to tables with diverse domains and a large number
of relations. We will delve deeper into extrapo-
lation performance (i.e., generalization to unseen
categories or topics) in x6.4.
In summary, preﬁx-tuning is an effective and
space-efﬁcient method to adapt GPT-2 to table-to-
text generation. It also maintains the performance
gains when scaling up to GPT-2 LARGE , suggesting
it has the potential to scale to even larger models
with a similar architecture, like GPT-3.
6.2 Summarization
As shown in Table 3, with 2% parameters, preﬁx-
tuning obtains slightly lower performance than ﬁne-
tuning (36.05 vs. 37.25 in ROUGE-L). With only
0.1% parameters, preﬁx-tuning underperforms full
ﬁne-tuning (35.05 vs. 37.25). There are several
differences between XSUM and the three table-to-
text datasets which could account for why preﬁx-
tuning has comparative advantage in table-to-text:
7250K for E2E, 250K for WebNLG, and 500K for DART
versus 345M GPT-2 parameters.
8The S,U,A columns in WebNLG represents SEEN, UN-
SEEN, and ALL respectively; SEEN categories appear at
training time; UNSEEN categories only appears at test time;
and A LL is the combination of the two.(1) XSUM contains 4x more examples than the
three table-to-text datasets on average; (2) the input
articles are 17x longer than the linearized table in-
put of table-to-text datasets on average; (3) summa-
rization is more complex than table-to-text because
it requires selecting key contents from an article.
6.3 Low-data Setting
Based on the results from table-to-text ( x6.1)
and summarization ( x6.2), we observe that preﬁx-
tuning has a comparative advantage when the num-
ber of training examples is smaller. To explore
the low-data setting more systematically, we sub-
sample the full dataset (E2E for table-to-text and
XSUM for summarization) to obtain small datasets
of sizef50;100;200;500g. For each size, we sam-
ple5different datasets and average over 2training
random seeds. Thus, we average over 10models
for each low-data setting.9
Figure 3 (right) shows that preﬁx-tuning outper-
forms ﬁne-tuning in low-data regimes by 2:9BLEU
on average, in addition to requiring much fewer pa-
rameters, but the gap narrows as the dataset size
increases.
Qualitatively, Figure 3 (left) shows 8examples
generated by both preﬁx-tuning and ﬁne-tuning
models trained on different data levels. While both
methods tend to undergenerate (missing table con-
tents) in low data regimes, preﬁx-tuning tends to be
more faithful than ﬁne-tuning. For example, ﬁne-
tuning (100, 200)10falsely claims a low customer
rating while the true rating is average, whereas
preﬁx-tuning (100, 200) generates a description
that is faithful to the table.
6.4 Extrapolation
We now investigate extrapolation performance to
unseen topics for both table-to-text and summariza-
tion. In order to construct an extrapolation setting,
we split the existing datasets so that training and
test cover different topics. For table-to-text, the
WebNLG dataset is labeled with table topics. There
are9categories that appear in training and dev, de-
noted as SEEN and 5categories that only appear at
test time, denoted as UNSEEN. So we evaluate ex-
trapolation by training on the SEEN categories and
testing on the UNSEEN categories. For summa-
rization, we construct two extrapolation data splits:
9We also sample a dev split (with dev size = 30% train-
ing size) for each training set. We use the dev split to choose
hyperparameters and perform early stopping.
10The number in the parenthesis refers to the training size.

4588E2E WebNLG DART
BLEU NIST MET R-L CIDEr BLEU MET TER # BLEU MET TER #Mover BERT BLEURT
S U A S U A S U A
GPT-2 MEDIUM
FT- FULL 68.8 8.71 46.1 71.1 2.43 64.7 26.7 45.7 0.46 0.30 0.38 0.33 0.78 0.54 46.2 0.39 0.46 0.50 0.94 0.39
FT- TOP2 68.1 8.59 46.0 70.8 2.41 53.6 18.9 36.0 0.38 0.23 0.31 0.49 0.99 0.72 41.0 0.34 0.56 0.43 0.93 0.21
ADAPTER (3%) 68.9 8.71 46.1 71.3 2.47 60.5 47.9 54.8 0.43 0.38 0.41 0.35 0.46 0.39 45.2 0.38 0.46 0.50 0.94 0.39
ADAPTER (0.1%) 66.3 8.41 45.0 69.8 2.40 54.5 45.1 50.2 0.39 0.36 0.38 0.40 0.46 0.43 42.4 0.36 0.48 0.47 0.94 0.33
PREFIX (0.1%) 70.3 8.82 46.3 72.1 2.46 62.9 45.3 55.0 0.44 0.37 0.41 0.35 0.51 0.42 46.4 0.38 0.46 0.50 0.94 0.39
GPT-2 LARGE
FT- FULL 68.5 8.78 46.0 69.9 2.45 65.3 43.1 55.5 0.46 0.38 0.42 0.33 0.53 0.42 47.0 0.39 0.46 0.51 0.94 0.40
Preﬁx 70.3 8.85 46.2 71.7 2.47 63.4 47.7 56.3 0.45 0.39 0.42 0.34 0.48 0.40 46.7 0.39 0.45 0.51 0.94 0.40
SOTA 68.6 8.70 45.3 70.8 2.37 63.9 52.8 57.1 0.46 0.41 0.44 - - - - - - - - -
Table 2: Metrics (higher is better, except for TER) for table-to-text generation on E2E (left), WebNLG (middle)
and DART (right). With only 0:1%parameters, Preﬁx-tuning outperforms other lightweight baselines and achieves
a comparable performance with ﬁne-tuning. The best score is boldfaced for both GPT-2 MEDIUM and GPT-2 LARGE .
Source name : The Eaglejtype : coffee shopjfood : Chinesejprice : cheapjcustomer
rating : averagejarea : riversidejfamily friendly : no jnear : Burger King
Preﬁx (50) The Eagle is a cheap Chinese coffee shop located near Burger King.
Preﬁx (100) The Eagle is a cheap coffee shop located in the riverside near Burger King. It
has average customer ratings.
Preﬁx (200) The Eagle is a cheap Chinese coffee shop located in the riverside area near
Burger King. It has average customer ratings.
Preﬁx (500) The Eagle is a coffee shop that serves Chinese food. It is located in the riverside
area near Burger King. It has an average customer rating and is not family
friendly.
FT (50) The Eagle coffee shop is located in the riverside area near Burger King.
FT (100) The Eagle is a cheap coffee shop near Burger King in the riverside area. It has
a low customer rating and is not family friendly.
FT (200) The Eagle is a cheap Chinese coffee shop with a low customer rating. It is
located near Burger King in the riverside area.
FT (500) The Eagle is a cheap Chinese coffee shop with average customer ratings. It is
located in the riverside area near Burger King.
100 200 300 400 500
training data size323334353637ROUGE-1
FT-full
Prefix
100 200 300 400 500
training data size101112131415ROUGE-2
FT-full
Prefix
100 200 300 400 500
training data size0.500.550.60BLEU
FT-full
Prefix
100 200 300 400 500
training data size0.600.620.640.66ROUGE
FT-full
Prefix
Figure 3: (Left) qualitative examples in lowdata settings. (Right) preﬁx-tuning (orange) outperforms ﬁne-tuning
(blue) in low-data regimes in addition to requiring many fewer parameters. The top two plots correspond to sum-
marization, measured by ROUGE-1 and ROUGE-2. The bottom two plots correspond to table-to-text, measured
by BLEU and ROUGE-L. The x-axis is the training size and the y-axis is the evaluation metric (higher is better).
R-1"R-2"R-L"
FT- FULL (Lewis et al., 2020) 45.14 22.27 37.25
PREFIX (2%) 43.80 20.93 36.05
PREFIX (0.1%) 42.92 20.03 35.05
Table 3: Performance of methods on the XSUM sum-
marization dataset. Preﬁx-tuning slightly underper-
forms ﬁne-tuning in the full-data regime.
news-to-sports within-news
R-1"R-2"R-L"R-1"R-2"R-L"
FT- FULL 38.15 15.51 30.26 39.20 16.35 31.15
PREFIX 39.23 16.74 31.51 39.41 16.87 31.47
Table 4: Extrapolation performance on XSUM. Preﬁx-
tuning outperforms ﬁne-tuning on both news-to-sports
and within-news splits.
Innews-to-sports , we train on news articles
and test on sports articles. In within-news , we
train onfworld, UK, business gnews and test on
the remaining news categories (e.g., health, tech).
On both table-to-text and summarization, preﬁx-
tuning extrapolates better than ﬁne-tuning under all
metrics, as shown in Table 4 and the ‘U’ columns
of Table 2 (middle).
We also ﬁnd that adapter-tuning achieves good
extrapolation performance, comparable with preﬁx-tuning, as shown in Table 2. This shared trend
suggests that preserving LM parameters indeed has
a positive impact on extrapolation. However, how
preﬁx-tuning improves extrapolation is an open
question and we will discuss this further in x8.
7 Intrinsic Evaluation
We compare different variants of preﬁx-tuning to
study the impact of various design decisions. x7.1
studies the impact of the preﬁx length. x7.2 studies
tuning only the embedding layer, which is more
akin to tuning a discrete prompt. x7.3 compares
preﬁxing and inﬁxing, which inserts trainable acti-
vations between xandy.x7.4 studies the impact of
various preﬁx initialization strategies. x7.5 further
studies the data efﬁciency of preﬁx-tuning.
7.1 Preﬁx Length
A longer preﬁx means more trainable parameters,
and therefore more expressive power.11Figure 4
shows that performance increases as the preﬁx
11Empirically, longer preﬁxes have a negligible impact on
training and inference speed per batch, because attention com-
putation over the entire preﬁx is parallellized on GPUs.

4589
0 100 200 300
Prefix Length (XSUM)18.519.019.520.020.521.0ROUGE-2
33.534.034.535.035.536.0
ROUGE-L
 ROUGE-2
ROUGE-L
0 10 20 30 40
Prefix Length (DART)44.044.545.045.546.0BLEU
0.4600.4650.4700.4750.480
TER
BLEU
TERFigure 4: Preﬁx length vs. performance on summer-
ization (left) and table-to-text (right). Performance in-
creases as the preﬁx length increases up to a threshold
(200 for summarization and 10 for table-to-text) and
then a slight performance drop occurs. Each plot re-
ports two metrics (on two vertical axes).
E2E
BLEU NIST MET ROUGE CIDEr
PREFIX 70.3 8.82 46.3 72.1 2.46
Embedding-only: E MB-fPreﬁxLengthg
EMB-1 48.1 3.33 32.1 60.2 1.10
EMB-10 62.2 6.70 38.6 66.4 1.75
EMB-20 61.9 7.11 39.3 65.6 1.85
Inﬁx-tuning: I NFIX -fPreﬁxLengthg
INFIX -1 67.9 8.63 45.8 69.4 2.42
INFIX -10 67.2 8.48 45.8 69.9 2.40
INFIX -20 66.7 8.47 45.8 70.0 2.42
Table 5: Intrinsic evaluation of Embedding-only ( x7.2)
and Inﬁxing (x7.3). Both Embedding-only ablation and
Inﬁx-tuning underperforms full preﬁx-tuning.
length increases up to a threshold ( 200for sum-
marization, 10for table-to-text) and then a slight
performance drop occurs. Preﬁxes longer than the
threshold lead to lower training loss, but slightly
worse test performance, suggesting that they tend
to overﬁt the training data.
7.2 Full vs Embedding-only
Recall inx4.1, we discussed optimizing the contin-
uous embeddings of the “virtual tokens.” We instan-
tiate that idea and call it embedding-only . The word
embeddings are free parameters, and the remaining
activation layers are computed by the Transformer.
Table 5 (top) shows that the performance drops
signiﬁcantly, suggesting that tuning only the em-
bedding layer is not sufﬁciently expressive.
Embedding-only upper bounds the performance
of discrete prompt optimization (Shin et al., 2020),
because discrete prompt restricts the embedding
layer to exactly match the embedding of a real word.
Consequently, we have this chain of increasing ex-
pressive power: discrete prompting <embedding-
only<preﬁx-tuning.
7.3 Preﬁx-tuning vs Inﬁx-tuning
We also investigate how the trainable activations’
position in the sequence affects performance. In
random "active""elephant""summarize""table-to-text:""banana" "beautiful""divide" "keep"0.450.500.550.60BLEUFigure 5: Initializing the preﬁx with activations of real
words signiﬁcantly outperforms random initialization,
in low-data settings.
preﬁx-tuning, we place them at the beginning
[PREFIX ;x;y]. We can also place the trainable
activations between xandy(i.e.[x;INFIX;y]) and
call this inﬁx-tuning. Table 5 (bottom) shows that
inﬁx-tuning slightly underperforms preﬁx-tuning.
We believe this is because preﬁx-tuning can affect
the activations of xandywhereas inﬁx-tuning can
only inﬂuence the activations of y.
7.4 Initialization
We ﬁnd that how the preﬁx is initialized has
a large impact in low-data settings. Random
initialization leads to low performance with high
variance. Initializing the preﬁx with activations of
real words signiﬁcantly improves generation, as
shown in Figure 5. In particular, initializing with
task relevant words such as “summarization” and
“table-to-text” obtains slightly better performance
than task irrelevant words such as “elephant”
and “divide”, but using real words is still better
than random. Moreover, in full data settings, the
initialization trick has no impact, and random
initialization leads to equally good performance.
Since we initialize the preﬁx with activations of
real words computed by the LM, this initialization
strategy is concordant with preﬁx-tuning’s philos-
ophy, which preserves the pretrained LM as much
as possible.
7.5 Data Efﬁciency
We also investigate the data efﬁciency of preﬁx-
tuning (without initialization trick, a.k.a random
initialization) and full ﬁne-tuning by comparing
their performance on 5 different data scales of the
E2E task (10%, 20%, 40%, 60%, and 80%). Fig-
ure 6 shows that preﬁx-tuning has better perfor-
mance than ﬁne-tuning when using more than 20%
of the data. For data scale of 10%, preﬁx-tuning
with random initialization yields comparable or
slightly lower performance than full ﬁne-tuning,

4590
20 40 60 80
percentage of training data6566676869BLEU
FT-full
Prefix
20 40 60 80
percentage of training data68697071ROUGE-L
FT-full
PrefixFigure 6: Data efﬁciency curves: percentage of train-
ing set vs. performance on table-to-text (E2E). Preﬁx-
tuning (without the initialization trick) is more data-
efﬁcient than ﬁne-tuning when using more than 20%
of the data.
necessitating the initialization trick ( x6.3) to im-
prove the performance in this low-data regime.
8 Discussion
We will discuss several favorable properties of
preﬁx-tuning and some open problems.
Personalization. As we note inx1, preﬁx-tuning
is advantageous when there are a large number
of tasks that needs to be trained independently.
One practical setting is user privacy (Shokri and
Shmatikov, 2015; McMahan et al., 2016). In order
to preserve user privacy, each user’s data needs to
be separated and a personalized model needs to be
trained independently for each user. Consequently,
each user can be regarded as an independent task. If
there are millions of users, preﬁx-tuning can scale
to this setting and maintain modularity, enabling
ﬂexible addition or deletion of users by adding or
deleting their preﬁxes without cross-contamination.
Batching across users. Under the same person-
alization setting, preﬁx-tuning allows batching dif-
ferent users’ queries even though they are backed
by different preﬁxes. When multiple users query
a cloud GPU device with their inputs, it is compu-
tationally efﬁcient to put these users in the same
batch. Preﬁx-tuning keeps the shared LM intact;
consequently, batching requires a simple step of
prepending the personalized preﬁx to user input,
and all the remaining computation is unchanged.
In contrast, we can’t batch across different users
in adapter-tuning, which has personalized adapters
between shared Transformer layers.
This batching beneﬁt could also help create efﬁ-
cient ensembles of multiple preﬁxes trained on the
same task (Lester et al., 2021).
Inductive bias of preﬁx-tuning. Recall that ﬁne-
tuning updates all pretrained parameters, whereas
preﬁx-tuning and adapter-tuning preserve them.Since the language models are pretrained on gen-
eral purpose corpora, preserving the LM parame-
ters might help generalization to domains unseen
during training. In concordance with this intuition,
we observe that both preﬁx-tuning and adapter-
tuning have signiﬁcant performance gain in extrap-
olation settings (x6.4); however, how these methods
improve extrapolation is an open question.
While preﬁx-tuning and adapter-tuning both
freeze the pretrained parameters, they tune different
sets of parameters to affect the activation layers of
the Transformer. Recall that preﬁx-tuning keeps the
LM intact and uses the preﬁx and the pretrained at-
tention blocks to affect the subsequent activations;
adapter-tuning inserts trainable modules between
LM layers, which directly add residual vectors to
the activations. Moreover, we observe that preﬁx-
tuning requires vastly fewer parameters compared
to adapter-tuning while maintaining comparable
performance. We think this gain in parameter efﬁ-
ciency is because preﬁx-tuning keeps the pretrained
LM intact as much as possible, and therefore ex-
ploits the LM more than adapter-tuning.
Recent work by Aghajanyan et al. (2020) uses
intrinsic dimension to show that there exists a low-
dimensional reparameterization that is as effective
for ﬁne-tuning as the full parametrization. This
explains why good accuracy on downstream tasks
can be obtained by updating only a small num-
ber of parameters. Our work echoes this ﬁnding
by showing that good generation performance can
also be attained by updating a very small preﬁx.
However, preﬁx-tuning is not just about the size of
trainable parameters, but more importantly, which
subset of parameters to modify. Therefore, it would
be interesting future work to explore other light-
weight ﬁne-tuning methods that achieve an even
better accuracy-size tradeoff.
Acknowledgments
We thank the members of p-lambda group as well
as anonymous reviewers for valuable feedback. We
gratefully acknowledge the support of a PECASE
award. XLL is supported by a Stanford Graduate
Fellowship.
Reproducibility
Our code is available at https://github.com/
XiangLi1999/PrefixTuning .
Experiments and data are available at https:
//worksheets.codalab.org/worksheets/
0x16e0c8e7ab1f4b22aaccddc8b586541f .

4591References
Armen Aghajanyan, Luke Zettlemoyer, and Sonal
Gupta. 2020. Intrinsic dimensionality explains the
effectiveness of language model ﬁne-tuning.
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of NLG systems. In
11th Conference of the European Chapter of the
Association for Computational Linguistics , Trento,
Italy. Association for Computational Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
Hung, Eric Frank, Piero Molino, Jason Yosinski, and
Rosanne Liu. 2020. Plug and play language mod-
els: A simple approach to controlled text generation.
InInternational Conference on Learning Represen-
tations .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. The WebNLG
challenge: Generating text from RDF data. In Pro-
ceedings of the 10th International Conference on
Natural Language Generation , pages 124–133, San-
tiago de Compostela, Spain. Association for Compu-
tational Linguistics.
Karen Hambardzumyan, Hrant Khachatrian, and
Jonathan May. 2021. WARP: word-level adversar-
ial reprogramming. CoRR , abs/2101.00121.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efﬁcient transfer learning for NLP.
InProceedings of the 36th International Conference
on Machine Learning , volume 97 of Proceedings
of Machine Learning Research , pages 2790–2799,
Long Beach, California, USA. PMLR.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what languagemodels know? Transactions of the Association for
Computational Linguistics , 8:423–438.
Mihir Kale. 2020. Text-to-text pre-training for data-to-
text tasks.
N. Keskar, B. McCann, L. R. Varshney, Caiming Xiong,
and R. Socher. 2019. Ctrl: A conditional trans-
former language model for controllable generation.
ArXiv , abs/1909.05858.
Ben Krause, Akhilesh Deepak Gotmare, Bryan Mc-
Cann, Nitish Shirish Keskar, Shaﬁq Joty, Richard
Socher, and Nazneen Fatema Rajani. 2020. GeDi:
Generative Discriminator Guided Sequence Genera-
tion. arXiv preprint arXiv:2009.06367 .
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation , StatMT ’07, pages 228–231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efﬁcient prompt
tuning.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 7871–7880, Online. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out , pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
Zhaojiang Lin, Andrea Madotto, and Pascale Fung.
2020. Exploring versatile generative language
model via parameter-efﬁcient transfer learning. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 441–459, Online. As-
sociation for Computational Linguistics.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
understands, too. arXiv preprint arXiv:2103.10385 .
Yang Liu and Mirella Lapata. 2019. Text summariza-
tion with pretrained encoders. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3730–3740, Hong Kong,
China. Association for Computational Linguistics.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising
pre-training for neural machine translation.

4592Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining ap-
proach. CoRR , abs/1907.11692.
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Con-
ference on Learning Representations .
H. Brendan McMahan, Eider Moore, Daniel Ramage,
and Blaise Ag ¨uera y Arcas. 2016. Federated learn-
ing of deep networks using model averaging. Pro-
ceedings of the 20 th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS) 2017 ,
abs/1602.05629.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
Topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , Brussels, Belgium.
Jekaterina Novikova, Ondrej Dusek, and Verena Rieser.
2017. The E2E dataset: New challenges for end-to-
end generation. CoRR , abs/1706.09254.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics , ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,
Kyunghyun Cho, and Iryna Gurevych. 2020.
Adapterfusion: Non-destructive task composition
for transfer learning.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying LMs with mixtures of soft prompts.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT) , Mexico City.
Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand
Sivaprasad, Chiachun Hsieh, Nazneen Fatema Ra-
jani, Xiangru Tang, Aadit Vyas, Neha Verma,
Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto,
Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori
Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu,
Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, and
Richard Socher. 2020. Dart: Open-domain struc-
tured data record to text generation.
A. Radford, Jeffrey Wu, R. Child, David Luan, Dario
Amodei, and Ilya Sutskever. 2019. Language mod-
els are unsupervised multitask learners.
Evani Radiya-Dixit and Xin Wang. 2020. How ﬁne can
ﬁne-tuning be? learning efﬁcient language models.
InProceedings of the Twenty Third International
Conference on Artiﬁcial Intelligence and Statistics ,volume 108 of Proceedings of Machine Learning Re-
search , pages 2435–2443, Online. PMLR.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea
Vedaldi. 2017. Learning multiple visual domains
with residual adapters. In Advances in Neural Infor-
mation Processing Systems , volume 30, pages 506–
516. Curran Associates, Inc.
Timo Schick and Hinrich Sch ¨utze. 2020. Exploiting
cloze questions for few shot text classiﬁcation and
natural language inference.
Thibault Sellam, Dipanjan Das, and Ankur Parikh.
2020. BLEURT: Learning robust metrics for text
generation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7881–7892, Online. Association for Computa-
tional Linguistics.
Sheng Shen, Daniel Fried, Jacob Andreas, and Dan
Klein. 2019. Pragmatically informative text gen-
eration. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4060–4067, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV
au2, Eric Wallace, and Sameer Singh. 2020. Auto-
prompt: Eliciting knowledge from language models
with automatically generated prompts.
Reza Shokri and Vitaly Shmatikov. 2015. Privacy-
preserving deep learning. In Proceedings of
the 22nd ACM SIGSAC Conference on Computer
and Communications Security , CCS ’15, page
1310–1321, New York, NY , USA. Association for
Computing Machinery.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and Ralph Weischedel. 2006. A study
of translation error rate with targeted human annota-
tion. In In Proceedings of the Association for Ma-
chine Transaltion in the Americas (AMTA 2006 .
Asa Cooper Stickland, Xian Li, and Marjan
Ghazvininejad. 2020. Recipes for adapting
pre-trained monolingual and multilingual models to
machine translation.
Nishant Subramani, Samuel R. Bowman, and
Kyunghyun Cho. 2020. Can unconditional lan-
guage models recover arbitrary sentences?
Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2015. Cider: Consensus-based image
description evaluation. In CVPR , pages 4566–4575.
IEEE Computer Society.

4593Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020.
Transformers: State-of-the-art natural language pro-
cessing. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing:
System Demonstrations , pages 38–45, Online. Asso-
ciation for Computational Linguistics.
Jeffrey O Zhang, Alexander Sax, Amir Zamir,
Leonidas Guibas, and Jitendra Malik. 2020a. Side-
tuning: A baseline for network adaptation via addi-
tive side networks.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020b. BERTScore:
Evaluating text generation with bert. In Interna-
tional Conference on Learning Representations .
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,
Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing
Liu, and Bill Dolan. 2020c. DIALOGPT : Large-
scale generative pre-training for conversational re-
sponse generation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations , pages 270–
278, Online. Association for Computational Linguis-
tics.
Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-
rich Sch ¨utze. 2020. Masking as an efﬁcient alterna-
tive to ﬁnetuning for pretrained language models.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M. Meyer, and Steffen Eger. 2019. MoverScore:
Text generation evaluating with contextualized em-
beddings and earth mover distance. In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 563–578, Hong
Kong, China. Association for Computational Lin-
guistics.
Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,
Xipeng Qiu, and Xuanjing Huang. 2020. Extrac-
tive summarization as text matching. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 6197–6208, On-
line. Association for Computational Linguistics.
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,
Wengang Zhou, Houqiang Li, and Tieyan Liu. 2020.
Incorporating bert into neural machine translation.
InInternational Conference on Learning Represen-
tations .

4594A Supplementary Material
A.1 Datasets and Metrics
We evaluate on three standard neural generation
datasets for the table-to-text task: E2E (Novikova
et al., 2017), WebNLG (Gardent et al., 2017), and
DART (Radev et al., 2020).
The E2E dataset contains approximately 50K ex-
amples with 8 distinct ﬁelds; it contains multiple
test references for one source table, and the average
output length is 22:9. We use the ofﬁcial evalua-
tion script,12which reports BLEU (Papineni et al.,
2002), NIST (Belz and Reiter, 2006), METEOR
(Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004),
and CIDEr (Vedantam et al., 2015).
The WebNLG (Gardent et al., 2017) dataset con-
sists of 22K examples, and the input xis a sequence
of (subject, property, object) triples. The average
output length is 22:5. In the training and validation
splits, the input describes entities from 9distinct
DBpedia categories (e.g., Monument). The test
split consists of two parts: the ﬁrst half contains
DB categories seen in training data, and the sec-
ond half contains 5unseen categories. These un-
seen categories are used to evaluate extrapolation.
We use the ofﬁcial evaluation script, which reports
BLEU, METEOR and TER (Snover et al., 2006).
DART (Radev et al., 2020) is an open domain
table-to-text dataset, with similar input format
(entity-relation-entity triples) as WebNLG. The av-
erage output length is 21:6. It consists of 82K ex-
amples from WikiSQL, WikiTableQuestions, E2E,
and WebNLG and applies some manual or auto-
mated conversion. We use the ofﬁcial evaluation
script13and report BLEU, METEOR, TER, Mover-
Score (Zhao et al., 2019), BERTScore (Zhang et al.,
2020b) and BLEURT (Sellam et al., 2020).
For the summarization task, we use the XSUM
(Narayan et al., 2018) dataset, which is an abstrac-
tive summarization dataset on news articles. There
are 225K examples. The average length of the ar-
ticles is 431 words and the average length of the
summaries is 23.3. We report ROUGE-1, ROUGE-
2 and ROUGE-L, computed by the python package
rouge-score .
Data pre-processing. For table-to-text, we lin-
earize a table xin order to ﬁt into a language model
context. In the E2E dataset, for example, “(ﬁeld A,
12https://github.com/tuetschek/
e2e-metrics
13https://github.com/Yale-LILY/dartvalue A), (ﬁeld B, value B)” is linearized to “ﬁeld
A : value Ajﬁeld B : value B”. Also, in WebNLG
and DART, a sequence of triple “(entity1.1, rela-
tion1, entity1.2), (entity2.1, relation2, entity2.2)”
is linearlized as “entity1.1 : relation1 : entity1.2 j
entity2.1 : relation2 : entity2.2”.
For summarization, we truncate the articles xto
512 BPE tokens.
Extrapolation data splits. We construct two ex-
trapolation data splits news-to-sports and
within-news from the original XSUM dataset.
XSUM dataset is drawn from BBC news, and we
identify the topic of each article based on its URL.
Since “news” and “sports” are the two domains
with the most articles, we create our ﬁrst train/test
split. Additionally, “news” has subdomains such as
“UK”, “world”, and “technology”. Consequently,
we create a second data split, using the top 3 news
subdomains (i.e.fworld, UK, business g) as train-
ing data and the rest as test data.
A.2 Hyperparameters
In Table 6, we report the hyperparameters used to
train the best-performing models documented in
the experiment section.
As for the search range of each hyperparameters:
the learning rates are selected from f1e-5, 5e-05,
8e-05g; the number of epochs are selected from f5,
10gfor table-to-text and f5, 25, 30gfor summa-
rization; We select the largest batch size that can ﬁt
into GPU memory and didn’t explicitly tune for an
optimal batch size. Preﬁx length are selected from
f1, 5, 10, 20, 40gfor table-to-text and f1, 10, 20,
50, 80, 100, 200, 300 gfor summarization. We use
perplexity and automatic generation metrics on the
validation set to select the best-performing models.
For table-to-text in the low data settings, we use a
learning rate of 5e-5, and a batch size of 10. We use
a preﬁx length of 6, since we apply the initialization
trick and initialize the preﬁx with “table-to-text:”,
which contains 6BPE tokens. Instead of tuning
the number of epochs, we tune the max steps of
updates inf100, 200, 400, 600 g, as shown in
Table 8. We apply early stopping based on the
performance of validation set, where the validation
size = 30% training size.
For summarization in the low data settings, we
use a learning rate of 5e-5 and a warmup step of
100. We use a batch size of 5for preﬁx-tuning
and6for ﬁne-tuning. We apply the initialization
trick and use the word “summarize” to initialize

4595learning rate # epoch batch size preﬁx length
Preﬁx:
E2E 8e-05 5 10 5
WebNLG 5e-05 5 5 5
DART 5e-05 10 5 10
XSUM 5e-05 30 14 100
Adapter:
E2E (3%) 5e-05 5 5 -
E2E (0.1%) 8e-05 10 5
WebNLG (3%) 5e-05 5 5 -
WebNLG (0.1%) 5e-05 10 5 -
DART (3%) 5e-05 5 5 -
DART (0.1%) 8e-05 5 5 -
Fine-tune:
E2E 5e-05 5 10 -
WebNLG 1e-05 10 6 -
DART 1e-05 10 6 -
FT-top2:
E2E 5e-05 5 10 -
WebNLG 5e-05 10 9 -
DART 5e-05 5 5 -
within-news
Fine-tune 3e-5 5 18 -
Preﬁx 5e-5 30 36 80
news-to-sports
Fine-tune 3e-5 5 18 -
Preﬁx 5e-5 15 36 40
Table 6: Hyperparameter settings for our method and
baseline methods.
R-1"R-2"R-L"
PREFIX (2%) 43.30 20.35 35.21
PREFIX (0.1%) 41.54 18.56 33.13
Table 7: Metrics for summarization on XSUM valida-
tion set.
the preﬁx, resulting in a preﬁx length of 1. We tune
the number of epochs in f3, 5, 10, 20, 30g, shown
in Table 8. We also apply early stopping based on
validation performance.
For the extrapolation setting, the hyperparame-
ters for our table-to-text model is the same as the
hyperparameters of WebNLG. The hyperparame-
ters for summarization is shown in the last block of
Table 6.
A.3 Validation Performance
Table 9 shows the validation performance on the
three table-to-text datasets. Table 7 shows the vali-
dation performance on XSUM.
size=50 size=100 size=200 size=500
Preﬁx (max steps) 200 200 200 400
Finetune (max steps) 100 100 200 400
Preﬁx (epoch) 30 20 20 20
Finetune (epoch) 30 10 3 3
Table 8: Max # update steps for low data settings.A.4 Additional Results for Low-data Settings
Figure 7 supplements the low-data performance
curves in Figure 3 by plotting the relationship be-
tween training size and generation metrics for both
preﬁx-tuning and ﬁne-tuning.
A.5 Additional Results for the Initialization
Experiment
Figure 8 supplements Figure 3 by plotting addi-
tional metrics for our initialization technique x7.4.
It validates that random initialization (from a uni-
form (0,1) distirbution) signiﬁcantly underperforms
initializing with real words; Additionally, initializ-
ing with task-relevant words (e.g., “summarization”
and “table-to-text”) attains slightly better gener-
ation scores than initializing with task-irrelevant
words (e.g., “elephant” and “banana”).
A.6 Qualitative Examples for Extrapolation
Table 10 contains qualitative examples from both
seen and unseen categories in WebNLG. We ﬁnd
that for unseen categories, both preﬁx-tuning and
ﬁne-tuning tend to undergenerate (generated out-
put do not cover full table contents) or generate
untruthfully (generated output is inconsistent with
table contents). In particular, preﬁx-tuning tends to
undergenerate whereas ﬁne-tuning tends to gener-
ate untruthfully. For seen categories, both perform
fairly well in terms of coverage and truthfulness.

4596E2E WebNLG DART
BLEU NIST MET R-L CIDEr BLEU MET TER #BLEU MET TER #Mover BERT BLEURT
GPT-2 MEDIUM
FT- FULL 74.2 8.76 49.3 76.9 2.66 66.03 0.47 0.30 50.46 0.41 0.44 0.52 0.95 0.41
FT- TOP2 72.7 8.51 48.2 75.3 2.60 54.61 0.39 0.47 48.41 0.39 0.48 0.48 0.94 0.33
ADAPTER (3%) 71.7 8.53 48.4 74.6 2.60 60.63 0.43 0.33 48.56 0.40 0.44 0.51 0.95 0.40
ADAPTER (0.1%) 68.1 8.30 45.9 71.4 2.41 53.24 0.40 0.39 44.72 0.38 0.47 0.47 0.94 0.35
PREFIX (0.1%) 74.8 8.80 49.4 76.8 2.69 64.52 0.46 0.32 51.11 0.41 0.43 0.52 0.95 0.42
GPT-2 LARGE
FT- FULL 72.1 8.62 48.5 75.1 2.56 64.69 0.46 0.31 51.00 0.41 0.44 0.52 0.95 0.43
Preﬁx 74.8 8.81 49.5 77.0 2.72 64.11 0.46 0.33 50.84 0.41 0.43 0.52 0.95 0.42
Table 9: Metrics on the development set (higher is better, except for TER) for table-to-text generation on E2E
(left), WebNLG (middle) and DART (right).
100 200 300 400 500
training_data_size323436rouge-1
method
FT
PT
100 200 300 400 500
training_data_size101112131415rouge-2
method
FT
PT
100 200 300 400 500
training_data_size242628rouge-L
method
FT
PT
100 200 300 400 500
training_data_size34567NIST
method
FT
PT
100 200 300 400 500
training_data_size0.320.340.360.38METEOR
method
FT
PT
100 200 300 400 500
training_data_size1.21.41.61.8CIDEr
method
FT
PT
Figure 7: Preﬁx-tuning (orange) outperforms ﬁne-tuning (blue) in low-data regimes in addition to requiring many
fewer parameters. The top three plots correspond to summarization, measured by ROUGE-1, ROUGE-2, and
ROUGE-L. The bottom three plots correspond to table-to-text, measured by NIST, METEOR, and CIDEr. The
x-axis is the training size and the y-axis is the evaluation metric (higher is better).
random init"active""elephant""summarize""table-to-text:""banana" "beautiful""divide" "keep"234567NIST
random init"active""elephant""summarize""table-to-text:""banana" "beautiful""divide" "keep"0.300.320.340.360.38METEOR
random init"active""elephant""summarize""table-to-text:""banana" "beautiful""divide" "keep"0.580.600.620.640.66ROUGE
random init"active""elephant""summarize""table-to-text:""banana" "beautiful""divide" "keep"1.01.21.41.61.8CIDEr
Figure 8: Initializing the preﬁx with activations of real words signiﬁcantly outperforms random initialization, in a
low-data setting with 100 training data.

4597Source [Unseen, Athelete] (Al Kharaitiyat SC, club, Alaa Abdul-Zahra), (Al Khor, ground, Al Kharaitiyat SC), (Shabab Al-Ordon Club,
club, Alaa Abdul-Zahra) (Amar Osim, manager, Al Kharaitiyat SC)
Preﬁx-tuning Al Kharaitiyat SC are managed by Amar Osim and play at their ground at Al Khor. Al Kharaitiyat SC are also
the club for which Alaa Abdul-Zahra is a player.
Fine-tuning Alaa Abdul-Zahra plays for Al-Kharaitiyat SC and Shabab Al-Ordon Club. He also plays for Al-Khor and
manages Al-Kharaitiyat SC.
Reference Alaa Abdul Zahra plays for Al Kharaitiyat SC which is located at Al Khor and managed by Amar Osim. The
Shabab Al-Ordon club is associated with Alaa Abdul-Zahra.
Source [Unseen, Transportation] (Genoa, location, Costa Crociere), (AIDA Cruises, operator, AIDAstella), (Costa Crociere, owner, AIDAstella)
Preﬁx-tuning AID Astella is operated by Aida Cruises and is owned by the Costa Rican tourist resort of Genoa.
Fine-tuning AID Astella, operated by AIDA-Cruises, is located in Genoa and is owned by the Costa Rican government.
Reference Costa Crociere is the owner of the AIDAstella and are based in Genoa. The operator of AIDAstella is AIDA
Cruises.
Source [Unseen, Politician] (Euro, currency, Netherlands), (Stellendam, birthPlace, Ab Klink ), (Netherlands, nationality, Ab Klink)
Preﬁx-tuning Ab Klink was born in Stellendam and is a national of the Netherlands where the currency is the Euro.
Fine-tuning Ab Klink is a national of the Netherlands where the currency is the Euro. He was born in Stellendam.
Reference Ab Klink was born in Stellendam in the Netherlands, where the national currency is the euro.
Source [Unseen, Politician] (Robert E, Lee, commander, Battle of Salem Church), (American Civil War, isPartOfMilitaryConﬂict, Battle of
Salem Church), (Battle of Salem Church, battles, Aaron S. Daggett)
Preﬁx-tuning Robert E. Lee was the commander of the Battle of Salem Church which was part of the military conﬂict in the
American Civil war.
Fine-tuning The Battle of Salem Church is part of the American Civil War and was commanded by Robert E. Lee.
Reference Robert E Lee was a commander in the Battle of Salem Church, which was one of the military conﬂicts in the
American Civil War. Aaron S Daggett fought in the same battle.
Source [Unseen, Artist] (Christian alternative rock, musicSubgenre, Alternative rock), (Alternative rock, genre, Andrew White (musi-
cian))
Preﬁx-tuning Andrew White is a Christian alternative rock musician.
Fine-tuning Andrew White, a Christian alternative rocker, performs.
Reference The musician Andrew White’s genre is alternative rock, the genre which has the sub genre Christian alternative
rock.
Source [Unseen, Artist] (Hip hop music, genre, Allen Forrest), (solo singer, background, Allen Forrest)
Preﬁx-tuning Allen Forrest is a solo singer.
Fine-tuning Born in
Reference Allen Forrest is a solo singer whose genre is Hip Hop music.
Source [Seen, ComicsCharacter] (Americans, nationality, Ducan Rouleau), (Ducan Rouleau, creator, Baymax),(Alan Tudyk, starring, Big Hero 6
(ﬁlm)), (Steven T Segle, creator, Baymax), (Big Hero 6 (ﬁlm), serires, Baymax)
Preﬁx-tuning Baymax is a character in Big Hero 6 which stars Alan Tudyk. He was created by Steven T. Seagle and the
American, Duncan Rouleau.
Fine-tuning Alan Tudyk stars in the ﬁlm Big Hero 6 in which Baymax is a character created by Steven T. Seagle and the
American, Duncan Rouleau.
Reference Baymax is a character who appeared in Big Hero 6 starring Alan Tudyk. It was created by Steven T Seagle and
the American, Duncan Rouleau.
Source [Seen, City] (Washington, D.C., capital, United States), (White Americans, ethnicGroup, United States), (United States,
country, New Jersey), (New York City, largest City, United States), (New Jersy, isPartOf, Atlantic City)
Preﬁx-tuning Washington D.C. is the capital of the United States where the largest city is New York City and the White
Americans are an ethnic group. Atlantic City, New Jersey is also part of the United States.
Fine-tuning Atlantic City, New Jersey is part of New Jersey in the United States. The capital city is Washington D.C. and
one of the ethnic groups is White Americans.
Reference New York City (NYC) is the largest U.S. city. Atlantic City, New Jersey are also part of the United States with
its capital as Washington, DC and home to White Americans.
Table 10: Qualitative examples from WebNLG. The ﬁrst 6 examples are from the unseen categories, labeled next
to source; the last two examples are from the seen categories. For unseen categories, both preﬁx-tuning and ﬁne-
tuning tend to undergenerate (generated output do not cover full table contents) or generate untruthfully (generated
output is inconsistent with table contents). In particular, preﬁx-tuning tends to undergenerate more often than
generate untruthfully whereas ﬁne-tuning tends to generate untruthfully. For seen categories, both perform fairly
well in terms of coverage and truthfulness.

