{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f25923c48ea4bc68583fb025ad0a781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
      "Daniel: Hello, Girafatron!\n",
      "Girafatron: Hello, Daniel.\n",
      "Daniel: I have an important question. I think it's very pertinent.\n",
      "Girafatron: Of course.\n",
      "Daniel: What is your favourite giraffe?\n",
      "Girafatron: Giraffes are not my favourite animals, but my favourite giraffe is a very special giraffe who lives in my garden.\n",
      "Daniel: A very special giraffe?\n",
      "Girafatron: Yes. He is not an ordinary giraffe, not a regular giraffe, but the giraffe of my garden.\n",
      "Daniel: Is he the most glorious giraffe in the world?\n",
      "Girafatron: Yes.\n",
      "Daniel\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     offload_folder=False,\n",
    "#     device_map=torch.device('cpu'),\n",
    "# )\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "    \n",
    "# )\n",
    "\n",
    "# sequences = pipeline(\n",
    "#    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "#     max_length=200,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")\n",
    "# output= \"\"\"\n",
    "#     Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
    "# Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
    "# Daniel: Hello, Girafatron!\n",
    "# Girafatron: Hello, Daniel.\n",
    "# Daniel: I have an important question. I think it's very pertinent.\n",
    "# Girafatron: Of course.\n",
    "# Daniel: What is your favourite giraffe?\n",
    "# Girafatron: Giraffes are not my favourite animals, but my favourite giraffe is a very special giraffe who lives in my garden.\n",
    "# Daniel: A very special giraffe?\n",
    "# Girafatron: Yes. He is not an ordinary giraffe, not a regular giraffe, but the giraffe of my garden.\n",
    "# Daniel: Is he the most glorious giraffe in the world?\n",
    "# Girafatron: Yes.\n",
    "# Daniel.\n",
    "#     \"\"\"\n",
    "    #this cell run for 127 minutes and 56 seconds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3d8497a1f54f8ea5260032c46cf1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afe758d030a4f0da41a60d18a1c7050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name='TheBloke/samantha-mistral-7B-GGUF'\n",
    "model_file='samantha-mistral-7b.Q4_K_M.gguf'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,model_file=model_file, hf=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b's revision is set to a version that doesn't leverage remote code (4e2d06f0a7c6370ebabbc30c6f59377ae8f73d76).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n",
      "The Falcon model was initialized without `trust_remote_code=True`, and will therefore leverage the transformers library implementation. tiiuae/falcon-7b's revision is set to a version that doesn't leverage remote code (4e2d06f0a7c6370ebabbc30c6f59377ae8f73d76).\n",
      "\n",
      "In order to override this, please set a revision manually or set `trust_remote_code=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6b7d1e560a47f6a1844297748554aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d38cd04983f46789bb338abf7cede4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matansharon/python/chat_with_docs_projects/some tests/first.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtiiuae/falcon-7b\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mpipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matansharon/python/chat_with_docs_projects/some%20tests/first.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:834\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 834\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    835\u001b[0m         model,\n\u001b[1;32m    836\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    837\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    838\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    839\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    840\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    841\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    844\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    845\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    271\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    570\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 571\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/models/falcon/modeling_falcon.py:997\u001b[0m, in \u001b[0;36mFalconPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[1;32m    982\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    994\u001b[0m ):\n\u001b[1;32m    995\u001b[0m     revision \u001b[39m=\u001b[39m sanitize_code_revision(pretrained_model_name_or_path, revision, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 997\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    998\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    999\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args,\n\u001b[1;32m   1000\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m   1001\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1002\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   1003\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1004\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1005\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1006\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1007\u001b[0m         use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m   1008\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1009\u001b[0m     )\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3301\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3291\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3294\u001b[0m     (\n\u001b[1;32m   3295\u001b[0m         model,\n\u001b[1;32m   3296\u001b[0m         missing_keys,\n\u001b[1;32m   3297\u001b[0m         unexpected_keys,\n\u001b[1;32m   3298\u001b[0m         mismatched_keys,\n\u001b[1;32m   3299\u001b[0m         offload_index,\n\u001b[1;32m   3300\u001b[0m         error_msgs,\n\u001b[0;32m-> 3301\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3302\u001b[0m         model,\n\u001b[1;32m   3303\u001b[0m         state_dict,\n\u001b[1;32m   3304\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3305\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3306\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3307\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3308\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3309\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3310\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3311\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3312\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3313\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3314\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3315\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3316\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3317\u001b[0m     )\n\u001b[1;32m   3319\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3320\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3675\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[39mif\u001b[39;00m shard_file \u001b[39min\u001b[39;00m disk_only_shard_files:\n\u001b[1;32m   3674\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 3675\u001b[0m state_dict \u001b[39m=\u001b[39m load_state_dict(shard_file)\n\u001b[1;32m   3677\u001b[0m \u001b[39m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[1;32m   3678\u001b[0m \u001b[39m# matching the weights in the model.\u001b[39;00m\n\u001b[1;32m   3679\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3680\u001b[0m     state_dict,\n\u001b[1;32m   3681\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3685\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3686\u001b[0m )\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:484\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m         map_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m    485\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    486\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/python/chat_with_docs_projects/.venv/lib/python3.10/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "with open('/Users/matansharon/python/chat_with_docs_projects/data/Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page in range(len(reader.pages)):\n",
    "        text += reader.pages[page].extract_text()\n",
    "# print(text)\n",
    "# print(len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
