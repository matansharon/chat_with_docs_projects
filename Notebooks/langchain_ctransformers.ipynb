{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_k': '40', 'top_p': '0.95', 'temperature': '0.8', 'repetition_penalty': '1.1', 'last_n_tokens': '64', 'seed': '-1', 'max_new_tokens': '256', 'stop': nan, 'stream': 'False', 'reset': 'True', 'batch_size': '8', 'threads': '-1', 'context_length': '-1', 'gpu_layers': '0'}\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_html('https://github.com/marella/ctransformers#config')\n",
    "df[1].head()\n",
    "config={}\n",
    "for i in range(len(df[1])):\n",
    "    row=df[1].iloc[i]\n",
    "    config[row['Parameter']]=row['Default']\n",
    "# config['temperature']=0.1\n",
    "# config['context_length']=4096\n",
    "# config['gpu_layers']=20\n",
    "\n",
    "print (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for CTransformers\n__root__\n  '>' not supported between instances of 'str' and 'int' (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mf:\\python\\chat_with_docs\\langchain_ctransformers.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/python/chat_with_docs/langchain_ctransformers.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m CTransformers(model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mF:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mpython\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mchat_with_docs\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mllama-2-7b-chat.Q4_K_M.gguf\u001b[39;49m\u001b[39m'\u001b[39;49m, model_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mllama\u001b[39;49m\u001b[39m'\u001b[39;49m,config\u001b[39m=\u001b[39;49mconfig)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/python/chat_with_docs/langchain_ctransformers.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m llm(\u001b[39m'\u001b[39m\u001b[39mhello world\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\load\\serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for CTransformers\n__root__\n  '>' not supported between instances of 'str' and 'int' (type=type_error)"
     ]
    }
   ],
   "source": [
    "llm = CTransformers(model='F:\\python\\chat_with_docs\\models\\llama-2-7b-chat.Q4_K_M.gguf', model_type='llama',config=config)\n",
    "llm('hello world')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=\"\"\"KNUMBER|APPLICANT|CONTACT|STREET1|STREET2|CITY|STATE|COUNTRY_CODE|ZIP|POSTAL_CODE|DATERECEIVED|DECISIONDATE|DECISION|REVIEWADVISECOMM|PRODUCTCODE|STATEORSUMM|CLASSADVISECOMM|SSPINDICATOR|TYPE|THIRDPARTY|EXPEDITEDREVIEW|DEVICENAME\n",
    "DEN000001|OHMEDA MEDICAL|DANIEL  KOSEDNAR|P.O. BOX 7550||MADISON|WI|US|53707-7550|53707-7550|01/07/2000|01/11/2000|DENG|AN|MRN||AN||Post-NSE|N||OHMEDA INOVENT DELIVERY SYSTEM\n",
    "K000001|BOSTON SCIENTIFIC SCIMED, INC.|RON  BENNETT|5905 NATHAN LN.||MINNEAPOLIS|MN|US|55442|55442|01/03/2000|06/05/2000|SESE|SU|JCT|Summary|SU||Traditional|N||WALLGRAFT TRACHEOBRONCHIAL ENDOPROSTHESIS AND UNISTEP DELIVERY SYSTEM\n",
    "DEN000002|UROSURGE, INC.|STEVEN J PREISS|2660 CROSSPARK RD.||CORALVILLE|IA|US|52241|52241|01/27/2000|02/09/2000|DENG|GU|NAM||GU||Post-NSE|N||UROSURGE PERCUTANEOUS SANS (STOLLER AFFERENT NERVE STIMULATOR) DEVICE\n",
    "K000002|USA INSTRUMENTS, INC.|RONY  THOMAS|1515 DANNER DR.||AURORA|OH|US|44202|44202|01/03/2000|02/23/2000|SESE|RA|MOS|Summary|RA||Traditional|N||MAGNA 5000 PHASED ARRAY CTL SPINE COIL\n",
    "\"\"\"\n",
    "llm.get_num_tokens(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\python\\chat_with_docs\\langchain_ctransformers.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/python/chat_with_docs/langchain_ctransformers.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm(\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\llms\\base.py:873\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    867\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    868\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    869\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    872\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 873\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    874\u001b[0m         [prompt],\n\u001b[0;32m    875\u001b[0m         stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    876\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    877\u001b[0m         tags\u001b[39m=\u001b[39mtags,\n\u001b[0;32m    878\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m    879\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    880\u001b[0m     )\n\u001b[0;32m    881\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    882\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    883\u001b[0m )\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\llms\\base.py:653\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    640\u001b[0m         )\n\u001b[0;32m    641\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    642\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    643\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    651\u001b[0m         )\n\u001b[0;32m    652\u001b[0m     ]\n\u001b[1;32m--> 653\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_helper(\n\u001b[0;32m    654\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39m(new_arg_supported), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    655\u001b[0m     )\n\u001b[0;32m    656\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\llms\\base.py:541\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    540\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 541\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    542\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    543\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\llms\\base.py:528\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    519\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    520\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    525\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    526\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    527\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 528\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    529\u001b[0m                 prompts,\n\u001b[0;32m    530\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    531\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    532\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    533\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    534\u001b[0m             )\n\u001b[0;32m    535\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    536\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    537\u001b[0m         )\n\u001b[0;32m    538\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\llms\\base.py:1048\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1045\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1046\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m   1047\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1048\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1049\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1050\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1051\u001b[0m     )\n\u001b[0;32m   1052\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[0;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\langchain\\llms\\ctransformers.py:104\u001b[0m, in \u001b[0;36mCTransformers._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m text \u001b[39m=\u001b[39m []\n\u001b[0;32m    103\u001b[0m _run_manager \u001b[39m=\u001b[39m run_manager \u001b[39mor\u001b[39;00m CallbackManagerForLLMRun\u001b[39m.\u001b[39mget_noop_manager()\n\u001b[1;32m--> 104\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient(prompt, stop\u001b[39m=\u001b[39mstop, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    105\u001b[0m     text\u001b[39m.\u001b[39mappend(chunk)\n\u001b[0;32m    106\u001b[0m     _run_manager\u001b[39m.\u001b[39mon_llm_new_token(chunk, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\llm\\lib\\site-packages\\ctransformers\\llm.py:566\u001b[0m, in \u001b[0;36mLLM._stream\u001b[1;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[0;32m    562\u001b[0m     stop \u001b[39m=\u001b[39m [stop]\n\u001b[0;32m    564\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(prompt)\n\u001b[1;32m--> 566\u001b[0m stop_regex \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39;49m(re\u001b[39m.\u001b[39;49mescape, stop)))\n\u001b[0;32m    567\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    568\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "llm('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
