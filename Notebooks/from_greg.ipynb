{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms.ctransformers import CTransformers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Type</th>\n",
       "      <th>Description</th>\n",
       "      <th>Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top_k</td>\n",
       "      <td>int</td>\n",
       "      <td>The top-k value to use for sampling.</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>top_p</td>\n",
       "      <td>float</td>\n",
       "      <td>The top-p value to use for sampling.</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>temperature</td>\n",
       "      <td>float</td>\n",
       "      <td>The temperature to use for sampling.</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>repetition_penalty</td>\n",
       "      <td>float</td>\n",
       "      <td>The repetition penalty to use for sampling.</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>last_n_tokens</td>\n",
       "      <td>int</td>\n",
       "      <td>The number of last tokens to use for repetitio...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Parameter   Type  \\\n",
       "0               top_k    int   \n",
       "1               top_p  float   \n",
       "2         temperature  float   \n",
       "3  repetition_penalty  float   \n",
       "4       last_n_tokens    int   \n",
       "\n",
       "                                         Description Default  \n",
       "0               The top-k value to use for sampling.      40  \n",
       "1               The top-p value to use for sampling.    0.95  \n",
       "2               The temperature to use for sampling.     0.8  \n",
       "3        The repetition penalty to use for sampling.     1.1  \n",
       "4  The number of last tokens to use for repetitio...      64  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_html('https://github.com/marella/ctransformers#config')\n",
    "df[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "res={}\n",
    "for i in range(len(df[1])):\n",
    "    row=df[1].iloc[i]\n",
    "    res[row['Parameter']]=row['Default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_k': '40', 'top_p': '0.95', 'temperature': 0.1, 'repetition_penalty': '1.1', 'last_n_tokens': '64', 'seed': '-1', 'max_new_tokens': '256', 'stop': nan, 'stream': 'False', 'reset': 'True', 'batch_size': '8', 'threads': '-1', 'context_length': 4096, 'gpu_layers': '0'}\n"
     ]
    }
   ],
   "source": [
    "res['temperature']=0.1\n",
    "res['context_length']=4096\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm=CTransformers(model='F:\\python\\chat_with_docs\\models\\llama-2-7b-chat.Q4_K_M.gguf',model_type='gguf')\n",
    "\n",
    "llm.config=res\n",
    "print(llm.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader('F:\\python\\chat_with_docs\\data\\QLoRA- Efficient Finetuning of Quantized LLMs.pdf')\n",
    "pages=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040\n"
     ]
    }
   ],
   "source": [
    "print(len(pages[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QL ORA: Efficient Finetuning of Quantized LLMs\n",
      "Tim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\n",
      "Luke Zettlemoyer\n",
      "University of Washington\n",
      "{dettmers,artidoro,ahai,lsz}@cs.washington.edu\n",
      "Abstract\n",
      "We present QLORA, an efficient finetuning approach that reduces memory us-\n",
      "age enough to finetune a 65B parameter model on a single 48GB GPU while\n",
      "preserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\n",
      "ents through a frozen, 4-bit quantized pretrained language model into Low Rank\n",
      "Adapters (LoRA). Our best model family, which we name Guanaco , outperforms\n",
      "all previous openly released models on the Vicuna benchmark, reaching 99.3%\n",
      "of the performance level of ChatGPT while only requiring 24 hours of finetuning\n",
      "on a single GPU. QLORAintroduces a number of innovations to save memory\n",
      "without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\n",
      "is information theoretically optimal for normally distributed weights (b) Double\n",
      "Quantization to reduce the average memory footprint by quantizing the quantization\n",
      "constants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\n",
      "to finetune more than 1,000 models, providing a detailed analysis of instruction\n",
      "following and chatbot performance across 8 instruction datasets, multiple model\n",
      "types (LLaMA, T5), and model scales that would be infeasible to run with regular\n",
      "finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\n",
      "finetuning on a small high-quality dataset leads to state-of-the-art results, even\n",
      "when using smaller models than the previous SoTA. We provide a detailed analysis\n",
      "of chatbot performance based on both human and GPT-4 evaluations showing that\n",
      "GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\n",
      "thermore, we find that current chatbot benchmarks are not trustworthy to accurately\n",
      "evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\n",
      "where Guanaco fails compared to ChatGPT. We release all of our models and code,\n",
      "including CUDA kernels for 4-bit training.2\n",
      "1 Introduction\n",
      "Finetuning large language models (LLMs) is a highly effective way to improve their performance,\n",
      "[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\n",
      "finetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\n",
      "parameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\n",
      "methods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\n",
      "inference and break down during training [65].\n",
      "We demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\n",
      "performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\n",
      "a pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\n",
      "∗Equal contribution.\n",
      "2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\n",
      "Preprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023\n"
     ]
    }
   ],
   "source": [
    "first_10_pages=''\n",
    "for i in range(1):\n",
    "    first_10_pages+=pages[i].page_content\n",
    "print(first_10_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQLORA (Quality of Life Oral Analysis) is a non-invasive, painless oral analysis device that measures the bacterial balance in your mouth. It is used to detect early signs of gum disease, bad breath, and other oral health issues before they become serious problems.\\nThe QLORA device uses advanced technology to analyze the bacteria in your mouth and provide a detailed report on your oral health. This information can be used by dentists and healthcare professionals to develop personalized treatment plans that promote good oral hygiene and overall well-being.\\nIn just seconds, the QLORA device can provide a comprehensive analysis of your oral bacteria, including:\\n* The types of bacteria present in your mouth\\n* The balance of bacteria (good vs. bad)\\n* The presence of harmful bacteria that can cause gum disease and other oral health issues\\nQLORA is a valuable tool for anyone looking to maintain good oral hygiene and prevent oral health problems. It is also a great way to monitor the effectiveness of your oral hygiene routine and make any necessary adjustments'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"what is qlora\")#before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAbstract:\\nIn this paper, we propose a novel method for fine-tuning quantized large language models (LLMs) called Efficient Finetuning of Quantized LLMs (EFT-Q). Our approach leverages the efficiency of quantization to reduce the computational requirements of fine-tuning, while maintaining the accuracy of the model. We evaluate EFT-Q on several benchmark datasets and show that it achieves competitive performance compared to state-of-the-art methods for quantized LLMs.\\nIntroduction:\\nWith the increasing adoption of low-bit devices in various applications, there is a growing need for efficient machine learning models that can be deployed on such devices. Quantization, which reduces the precision of weights and activations in neural networks, is a popular technique to reduce the computational requirements of these models. However, fine-tuning quantized models can result in reduced accuracy due to the loss of information during the quantization process. In this paper, we propose Efficient Finetuning of Quantized LLMs (EFT-Q), a method that leverages the efficiency of quantization to reduce the computational requirements of fine-tuning while maintaining the accuracy of the model.\\nMethod'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(f\"summerize this {pages[0].page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
