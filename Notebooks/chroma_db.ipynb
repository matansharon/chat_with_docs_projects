{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"F:\\python\\chat_with_docs\\data\\qlora.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n∗Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023', metadata={'source': 'F:\\\\python\\\\chat_with_docs\\\\data\\\\qlora.pdf', 'page': 0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pages:\n",
    "    print(len(i.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils import embedding_functions\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "out=sentence_transformer_ef('boo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('F:\\python\\chat_with_docs\\data\\Self supervised learning tuturial slides.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "text=''\n",
    "for doc in documents:\n",
    "    text+=doc.page_content+'\\n\\n\\n'\n",
    "# db = Chroma.from_documents(documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emmbedding=sentence_transformer_ef(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\python\\chat_with_docs\\Notebooks\\chroma_db.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/python/chat_with_docs/Notebooks/chroma_db.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m db \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(documents, emmbedding)\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\.venv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:646\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    645\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 646\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(\n\u001b[0;32m    647\u001b[0m     texts\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    648\u001b[0m     embedding\u001b[39m=\u001b[39membedding,\n\u001b[0;32m    649\u001b[0m     metadatas\u001b[39m=\u001b[39mmetadatas,\n\u001b[0;32m    650\u001b[0m     ids\u001b[39m=\u001b[39mids,\n\u001b[0;32m    651\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    652\u001b[0m     persist_directory\u001b[39m=\u001b[39mpersist_directory,\n\u001b[0;32m    653\u001b[0m     client_settings\u001b[39m=\u001b[39mclient_settings,\n\u001b[0;32m    654\u001b[0m     client\u001b[39m=\u001b[39mclient,\n\u001b[0;32m    655\u001b[0m     collection_metadata\u001b[39m=\u001b[39mcollection_metadata,\n\u001b[0;32m    656\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    657\u001b[0m )\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\.venv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:601\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[0;32m    570\u001b[0m     \u001b[39mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    581\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Chroma:\n\u001b[0;32m    582\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \n\u001b[0;32m    584\u001b[0m \u001b[39m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m     chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    602\u001b[0m         collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    603\u001b[0m         embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[0;32m    604\u001b[0m         persist_directory\u001b[39m=\u001b[39mpersist_directory,\n\u001b[0;32m    605\u001b[0m         client_settings\u001b[39m=\u001b[39mclient_settings,\n\u001b[0;32m    606\u001b[0m         client\u001b[39m=\u001b[39mclient,\n\u001b[0;32m    607\u001b[0m         collection_metadata\u001b[39m=\u001b[39mcollection_metadata,\n\u001b[0;32m    608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m     chroma_collection\u001b[39m.\u001b[39madd_texts(texts\u001b[39m=\u001b[39mtexts, metadatas\u001b[39m=\u001b[39mmetadatas, ids\u001b[39m=\u001b[39mids)\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mf:\\python\\chat_with_docs\\.venv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:127\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persist_directory \u001b[39m=\u001b[39m (\n\u001b[0;32m    121\u001b[0m         _client_settings\u001b[39m.\u001b[39mpersist_directory \u001b[39mor\u001b[39;00m persist_directory\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39m=\u001b[39m embedding_function\n\u001b[0;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mget_or_create_collection(\n\u001b[0;32m    126\u001b[0m     name\u001b[39m=\u001b[39mcollection_name,\n\u001b[1;32m--> 127\u001b[0m     embedding_function\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    130\u001b[0m     metadata\u001b[39m=\u001b[39mcollection_metadata,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverride_relevance_score_fn \u001b[39m=\u001b[39m relevance_score_fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "db = Chroma.from_documents(documents, emmbedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
