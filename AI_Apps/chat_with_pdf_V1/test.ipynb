{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splited_docs 101\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import TextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "import json\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DIRECTORY_PATH='/Users/matansharon/python/chat_with_doc/AI_Apps/chat_with_pdf/data'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_documents_names():\n",
    "    documents_names = os.listdir(DIRECTORY_PATH)\n",
    "    res=[]\n",
    "    for doc in documents_names:\n",
    "        if doc.endswith(\".pdf\"):\n",
    "            res.append(doc)\n",
    "    return res\n",
    "def load_all_docs_in_data_folder(documents_names):\n",
    "    \n",
    "    data_documents= []\n",
    "    for doc in documents_names:\n",
    "        path=os.path.join(DIRECTORY_PATH+'/',doc)\n",
    "        Document=PyPDF2.PdfReader(path)\n",
    "        text = ''\n",
    "        for page in Document.pages:\n",
    "            text += page.extract_text()\n",
    "        data_documents.append(text)\n",
    "        \n",
    "    return data_documents\n",
    "    \n",
    "    \n",
    "\n",
    "def split_text(text:str):\n",
    "    print(\"in split\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks=text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def create_new_db(chunks):\n",
    "    print(\"in create db\")\n",
    "    \n",
    "    path='/Users/matansharon/python/chat_with_doc/AI_Apps/chroma_db'\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "        db=Chroma.from_texts(texts=[''],embedding=OpenAIEmbeddings(model='text-embedding-3-small'),persist_directory=path)\n",
    "        return db\n",
    "    if chunks:\n",
    "        \n",
    "        db=Chroma.from_texts(texts=chunks,embedding=OpenAIEmbeddings(model='text-embedding-3-small'),persist_directory=path)\n",
    "        return db\n",
    "    return load_db()\n",
    "\n",
    "def load_db():\n",
    "    \n",
    "    db = Chroma(persist_directory='/Users/matansharon/python/chat_with_doc/AI_Apps/chroma_db',embedding_function=OpenAIEmbeddings(model='text-embedding-3-small'))\n",
    "    return db\n",
    "\n",
    "def get_results_with_scores(query,db):\n",
    "    print(\"in get results with scores\")\n",
    "    res=db.similarity_search_with_relevance_scores(query,k=3)\n",
    "    value_results=[]\n",
    "    \n",
    "    for i in res:\n",
    "        score=i[1]\n",
    "        if score>0 and score<0.5:\n",
    "            value_results.append(i)\n",
    "    return value_results\n",
    "def get_prompt_template(results,query):\n",
    "    if len(results)==0:\n",
    "        # template=\"\"\"\n",
    "        # start the answer with the sentence: \"i don't have any context to rely on so i give you the answer base on my previous knowledge.\"\n",
    "        # and then add you answer for this {query}\n",
    "        # \"\"\"\n",
    "        # return template\n",
    "        return query\n",
    "    template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "    context_texts = []\n",
    "    for i in range(len(results)):\n",
    "        context_texts.append(results[i][0].page_content)\n",
    "    temp = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "    res=prompt_tamplate.format(context=temp,query=query)\n",
    "    return res\n",
    "\n",
    "def get_response(query,db,model):\n",
    "    results=get_results_with_scores(query,db)\n",
    "    prompt_template=get_prompt_template(results,query)\n",
    "    response=model.invoke(prompt_template)\n",
    "    return response.content\n",
    "def add_document(document,db):\n",
    "    \n",
    "    text=\"\"\n",
    "    for page in document.pages:\n",
    "        text+=page.extract_text()\n",
    "    chunks=split_text(text)\n",
    "    db.add_texts(chunks)\n",
    "    \n",
    "def main_app():\n",
    "    print(\"in main app\")\n",
    "    documents_names=get_documents_names()\n",
    "    docs=load_all_docs_in_data_folder(documents_names=documents_names)\n",
    "    splited_docs=[]\n",
    "    for doc in docs:\n",
    "        chunks=split_text(doc)\n",
    "        for chunk in chunks:\n",
    "            splited_docs.append(chunk)\n",
    "            \n",
    "    print(len(splited_docs))\n",
    "    db=create_new_db(splited_docs)\n",
    "    # model=ChatOpenAI(model='gpt-4-turbo-preview')\n",
    "    model=ChatOpenAI()\n",
    "    \n",
    "    return db,model,documents_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matansharon/python/chat_with_docs/.venv/lib/python3.10/site-packages/langchain_core/vectorstores.py:331: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa—in other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.'), -0.2416019029400016), (Document(page_content='benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa—in other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.'), -0.24161159671515908), (Document(page_content='benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa—in other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.'), -0.2416398351036615)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Lionel Messi is a professional footballer from Argentina who is widely considered one of the greatest players of all time. He has won numerous awards and accolades throughout his career, including multiple FIFA Ballon d'Or titles. Messi has played for FC Barcelona for the majority of his career before moving to Paris Saint-Germain in 2021.\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\n    answer the question base only on the following context:\\n    Our QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\n\\n---\\n\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have\\n\\n---\\n\\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the\\ninstructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\\nlearning rate for 33B and 65B while doubling the batch size.\\n    answer the question base on the above context: what is Finetuning?\\n    \\n    '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp=get_prompt_template(get_results_with_scores('what is Finetuning?',db),'what is Finetuning?')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_results_with_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is qlora?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 74\u001b[0m, in \u001b[0;36mget_results_with_scores\u001b[0;34m(query, db)\u001b[0m\n\u001b[1;32m     71\u001b[0m value_results\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m---> 74\u001b[0m     score\u001b[38;5;241m=\u001b[39m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m score\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m     76\u001b[0m         value_results\u001b[38;5;241m.\u001b[39mappend(res[i])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "get_results_with_scores(\"what is qlora?\",db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Finetuning is the process of adjusting and optimizing a pre-trained language model by further training it on a specific dataset or task, in order to improve its performance on that particular task. In the context provided, finetuning is used to transform raw pretrained large language models (LLMs) into chatbot models like ChatGPT, with the goal of achieving state-of-the-art natural language processing (NLP) technology.')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There is no widely known or established meaning for the term \"qlora.\" It is possible that it could be a unique or made-up word with no specific definition. If you have more context or information about the term, please provide it so that a more accurate answer can be given.')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=ChatOpenAI()\n",
    "model2.invoke(\"what is qlora?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matansharon/python/chat_with_docs/.venv/lib/python3.10/site-packages/langchain_core/vectorstores.py:331: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='pairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\\nrelative to an opponent’s win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K= 32 .'), -0.2526158859802625), (Document(page_content='pairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\\nrelative to an opponent’s win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K= 32 .'), -0.2526312274331204), (Document(page_content='pairs of models compete to produce the best response for a given prompt. This is similar to how Bai\\net al. [4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to\\nhuman ratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17].\\nElo rating, which is widely used in chess and other games, is a measure of the expected win-rate\\nrelative to an opponent’s win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player\\nhas an expected win-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or\\n1100 vs 1100 match results in an expected win-rate of 50%. The Elo rating changes after each match\\nproportionally to the expected outcome, that is, an unexpected upset leads to a large change in Elo\\nrating while an expected outcome leads to a small change. Over time, Elo ratings approximately\\nmatch the skill of each player at playing the game. We start with a score of 1,000 and use K= 32 .'), -0.2526312274331204)]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have any context to rely on so I give you the answer based on my previous knowledge. In situations like this, it's important to gather more information before making any decisions.\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"who is messi?\",db,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chunks)\n",
    "\n",
    "# model=ChatOpenAI()\n",
    "# query='what is qlora?'\n",
    "\n",
    "# response=get_response(query,db,model)\n",
    "# print(response)\n",
    "# db=create_new_db('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from PyPDF2 import PdfReader\n",
    "path='/Users/matansharon/python/chat_with_docs/data/pdf/qlora.pdf'\n",
    "doc=PdfReader(path)\n",
    "#get the name of the document\n",
    "doc_name=path.split('/')[-1]\n",
    "\n",
    "text=''\n",
    "for page in doc.pages:\n",
    "    text+=page.extract_text()\n",
    "\n",
    "\n",
    "\n",
    "chunks=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100,length_function=len,add_start_index=True).split_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response('who is the authors of the paper \"Attention Is All You Need\"',db,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat=ChatOpenAI(model='gpt-4-turbo-preview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.invoke(f'base on this: {content} what is qlora?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
