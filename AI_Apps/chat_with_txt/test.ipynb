{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is Pixegami tutorial\n",
    "### from this video :https://www.youtube.com/watch?v=tcqEUSNCn8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "import json\n",
    "load_dotenv()\n",
    "directory_path='/Users/matansharon/python/chat_with_docs/data/text'\n",
    "def get_all_docs():\n",
    "    path='/Users/matansharon/python/chat_with_docs/AI_Apps/chat_with_txt/docs.json'\n",
    "    with open(path,'r') as f:\n",
    "        data=json.load(f)\n",
    "        docs=data['docs']\n",
    "    return docs\n",
    "def load_and_split_documents():\n",
    "    loader = DirectoryLoader(directory_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks=text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "def create_db(chunks):\n",
    "    path='chroma_db'\n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "        db=Chroma.from_documents(documents=chunks,embedding=OpenAIEmbeddings(),persist_directory=path)\n",
    "        return db\n",
    "    return load_db()\n",
    "def load_db():\n",
    "    db = Chroma(persist_directory=\"chroma_db\",embedding_function=OpenAIEmbeddings())\n",
    "    return db\n",
    "def get_results_with_scores(query,db):\n",
    "    bar=0.5\n",
    "    res=db.similarity_search_with_relevance_scores(query,k=3)\n",
    "    \n",
    "    return res\n",
    "def get_prompt_template(results,query):\n",
    "    template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "    context_texts = []\n",
    "    for i in range(len(results)):\n",
    "        context_texts.append(results[i][0].page_content)\n",
    "    temp = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "    res=prompt_tamplate.format(context=temp,query=query)\n",
    "    return res\n",
    "def get_response(query,db,model):\n",
    "    results=get_results_with_scores(query,db)\n",
    "    prompt_template=get_prompt_template(results,query)\n",
    "    response=model.invoke(prompt_template)\n",
    "    return response.content\n",
    "def main_app():\n",
    "    # pass\n",
    "    chunks=load_and_split_documents()\n",
    "    db=create_db(chunks)\n",
    "    #for GPT-4 use this: 'gpt-4-turbo-preview'\n",
    "    model=ChatOpenAI()\n",
    "    \n",
    "    return db,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "path='/Users/matansharon/python/chat_with_docs/data/pdf/qlora.pdf'\n",
    "doc=PyPDFLoader(path).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100,length_function=len,add_start_index=True).split_documents(doc)\n",
    "splited_str=[]\n",
    "for doc in chunks:\n",
    "    splited_str.append(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the paper \"Attention Is All You Need\" are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.\n"
     ]
    }
   ],
   "source": [
    "print(get_response('who is the authors of the paper \"Attention Is All You Need\"',db,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='As of my last update in 2023, \"Qlora\" does not appear to be a widely recognized term or brand in major global contexts, including technology, science, health, or entertainment. It\\'s possible that \"Qlora\" could be a specific product, service, or concept that is localized, newly introduced, or not broadly known outside a particular niche or industry.\\n\\nIf you are referring to a specific product, brand, or concept named \"Qlora,\" it would be helpful to have more context or details to provide a more accurate and helpful response. It\\'s also possible that new developments related to \"Qlora\" could have emerged after my last update. Please check the most recent sources for the latest information.'\n"
     ]
    }
   ],
   "source": [
    "chat=ChatOpenAI(model='gpt-4-turbo-preview')\n",
    "print(chat.invoke('what is qlora?\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2=Chroma.from_documents(documents=chunks,embedding=OpenAIEmbeddings(),persist_directory='chorma_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLORA is an efficient finetuning approach designed to reduce memory usage significantly, enabling the finetuning of a 65B parameter model on a single 48GB GPU while maintaining full 16-bit finetuning task performance. It operates by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The approach incorporates several innovations to conserve memory without compromising performance, including the introduction of a new data type (4-bit NormalFloat), Double Quantization to save on bits per parameter, and Paged Optimizers to manage memory spikes. QLORA allows for an extensive study of instruction finetuning and chatbot performance across a wide range of model sizes and architectures, making the finetuning of high-quality Large Language Models (LLMs) more accessible and widely available.\n"
     ]
    }
   ],
   "source": [
    "print(get_response('what is qlora?',db2,chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attention', 'pytorch', 'qlora']\n"
     ]
    }
   ],
   "source": [
    "#read the json file\n",
    "import json\n",
    "with open('docs.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(data['documents'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
