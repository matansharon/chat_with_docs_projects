{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is Pixegami tutorial\n",
    "### from this video :https://www.youtube.com/watch?v=tcqEUSNCn8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "directory_path='/Users/matansharon/python/chat_with_docs/data/text'\n",
    "\n",
    "def load_and_split_documents():\n",
    "    loader = DirectoryLoader(directory_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks=text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "def create_db(chunks):\n",
    "    path='chroma_db'\n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "        db=Chroma.from_documents(documents=chunks,embedding=OpenAIEmbeddings(),persist_directory=path)\n",
    "        return db\n",
    "    return load_db()\n",
    "def load_db():\n",
    "    db = Chroma(persist_directory=\"chroma_db\",embedding_function=OpenAIEmbeddings())\n",
    "    return db\n",
    "def get_results_with_scores(query,db):\n",
    "    bar=0.5\n",
    "    res=db.similarity_search_with_relevance_scores(query,k=3)\n",
    "    if len(res)==0 or res[0][1]<bar:\n",
    "        return 'No results found'\n",
    "    r=[]\n",
    "    for i in range(len(res)):\n",
    "        if res[i][1]>bar:\n",
    "            r.append(res[i])\n",
    "    return r\n",
    "def get_prompt_template(results,query):\n",
    "    template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "    context_texts = []\n",
    "    for result in results:\n",
    "        try:\n",
    "            \n",
    "            context_texts.append(result[0].page_content)\n",
    "        except Exception as error:\n",
    "            # Handle the error or skip the item\n",
    "            print(f\"Skipping item due to error: {datetime.now()}\\n and the error is: {error}\")\n",
    "            \n",
    "            break\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "    res=prompt_tamplate.format(context=context_text,query=query)\n",
    "    return res\n",
    "def get_response(query,db,model):\n",
    "    results=get_results_with_scores(query,db)\n",
    "    prompt_template=get_prompt_template(results,query)\n",
    "    response=model.invoke(prompt_template)\n",
    "    return response\n",
    "def main_app():\n",
    "\n",
    "    db=load_db()\n",
    "    model=ChatOpenAI()\n",
    "    return db,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=create_db(load_and_split_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "db.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLORA is an efficient finetuning approach that reduces memory usage, allowing for the finetuning of large language models on a single GPU while maintaining high performance. It involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family resulting from QLORA, named Guanaco, outperforms previously released models on the Vicuna benchmark. Additionally, QLORA introduces innovations such as a new data type called 4-bit NormalFloat (NF4) and a Double---processing technique to save memory without sacrificing performance.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what is qlora?\"\n",
    "response=get_response(query,db,model)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "    answer the question base only on the following context:\n",
      "    QL ORA: Efficient Finetuning of Quantized LLMs\n",
      "\n",
      "Tim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\n",
      "\n",
      "Luke Zettlemoyer\n",
      "\n",
      "University of Washington\n",
      "\n",
      "{dettmers,artidoro,ahai,lsz}@cs.washington.edu\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present QLORA, an efficient finetuning approach that reduces memory us-\n",
      "\n",
      "age enough to finetune a 65B parameter model on a single 48GB GPU while\n",
      "\n",
      "preserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\n",
      "\n",
      "ents through a frozen, 4-bit quantized pretrained language model into Low Rank\n",
      "\n",
      "Adapters (LoRA). Our best model family, which we name Guanaco , outperforms\n",
      "\n",
      "all previous openly released models on the Vicuna benchmark, reaching 99.3%\n",
      "\n",
      "of the performance level of ChatGPT while only requiring 24 hours of finetuning\n",
      "\n",
      "on a single GPU. QLORAintroduces a number of innovations to save memory\n",
      "\n",
      "without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\n",
      "\n",
      "is information theoretically optimal for normally distributed weights (b) Double\n",
      "\n",
      "---\n",
      "\n",
      "processing a mini-batch with a long sequence length. We combine these contributions into a better\n",
      "\n",
      "tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\n",
      "\n",
      "the accuracy tradeoffs seen in prior work.\n",
      "\n",
      "QLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\n",
      "\n",
      "performance on model scales that would be impossible using regular finetuning due to memory\n",
      "\n",
      "overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\n",
      "\n",
      "model architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\n",
      "\n",
      "recovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also\n",
      "\n",
      "analyze trends in the trained models. First, we find that data quality is far more important than\n",
      "\n",
      "dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\n",
      "\n",
      "---\n",
      "\n",
      "large corporations and small teams with consumer GPUs.\n",
      "\n",
      "Another potential source of impact is deployment to mobile phones. We believe our QLORAmethod\n",
      "\n",
      "might enable the critical milestone of enabling the finetuning of LLMs on phones and other low\n",
      "\n",
      "resource settings. While 7B models were shown to be able to be run on phones before, QLORAis\n",
      "\n",
      "the first method that would enable the finetuning of such models. We estimate that with an iPhone 12\n",
      "\n",
      "Plus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\n",
      "\n",
      "7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\n",
      "\n",
      "novel applications that have not been possible before due to privacy or LLM quality issues. QLORA\n",
      "\n",
      "can help enable privacy-preserving usage of LLMs, where users can own and manage their own data\n",
      "\n",
      "and models, while simultaneously making LLMs easier to deploy.\n",
      "\n",
      "However, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\n",
      "    answer the question base on the above context: what is qlora?\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "results=get_results_with_scores(query,db)\n",
    "# print(results)\n",
    "def get_prompt_template(results,query):\n",
    "    template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "    context_texts = []\n",
    "    for result in results:\n",
    "        try:\n",
    "            \n",
    "            context_texts.append(result[0].page_content)\n",
    "        except Exception as error:\n",
    "            # Handle the error or skip the item\n",
    "            print(f\"Skipping item due to error: {datetime.now()}\\n and the error is: {error}\")\n",
    "            print(result[0].page_content)\n",
    "            \n",
    "            break\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "    res=prompt_tamplate.format(context=context_text,query=query)\n",
    "    return res\n",
    "p=get_prompt_template(results,query)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QL ORA: Efficient Finetuning of Quantized LLMs\\n\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\n\\nLuke Zettlemoyer\\n\\nUniversity of Washington\\n\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\n\\nAbstract\\n\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\n\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\n\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\n\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\n\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\n\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\n\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\n\\non a single GPU. QLORAintroduces a number of innovations to save memory\\n\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\n\\nis information theoretically optimal for normally distributed weights (b) Double'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=results[0]\n",
    "res[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "context_texts = []\n",
    "for result in results:\n",
    "    try:\n",
    "        res, _score = result\n",
    "        context_texts.append(res.page_content)\n",
    "    except ValueError:\n",
    "        # Handle the error or skip the item\n",
    "        print(f\"Skipping item due to error: {result}\")\n",
    "context_text = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "res=prompt_tamplate.format(context=context_text,query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "    answer the question base only on the following context:\n",
      "    QL ORA: Efficient Finetuning of Quantized LLMs\n",
      "\n",
      "Tim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\n",
      "\n",
      "Luke Zettlemoyer\n",
      "\n",
      "University of Washington\n",
      "\n",
      "{dettmers,artidoro,ahai,lsz}@cs.washington.edu\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present QLORA, an efficient finetuning approach that reduces memory us-\n",
      "\n",
      "age enough to finetune a 65B parameter model on a single 48GB GPU while\n",
      "\n",
      "preserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\n",
      "\n",
      "ents through a frozen, 4-bit quantized pretrained language model into Low Rank\n",
      "\n",
      "Adapters (LoRA). Our best model family, which we name Guanaco , outperforms\n",
      "\n",
      "all previous openly released models on the Vicuna benchmark, reaching 99.3%\n",
      "\n",
      "of the performance level of ChatGPT while only requiring 24 hours of finetuning\n",
      "\n",
      "on a single GPU. QLORAintroduces a number of innovations to save memory\n",
      "\n",
      "without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\n",
      "\n",
      "is information theoretically optimal for normally distributed weights (b) Double\n",
      "\n",
      "---\n",
      "\n",
      "processing a mini-batch with a long sequence length. We combine these contributions into a better\n",
      "\n",
      "tuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\n",
      "\n",
      "the accuracy tradeoffs seen in prior work.\n",
      "\n",
      "QLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\n",
      "\n",
      "performance on model scales that would be impossible using regular finetuning due to memory\n",
      "\n",
      "overhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\n",
      "\n",
      "model architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\n",
      "\n",
      "recovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also\n",
      "\n",
      "analyze trends in the trained models. First, we find that data quality is far more important than\n",
      "\n",
      "dataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\n",
      "\n",
      "---\n",
      "\n",
      "large corporations and small teams with consumer GPUs.\n",
      "\n",
      "Another potential source of impact is deployment to mobile phones. We believe our QLORAmethod\n",
      "\n",
      "might enable the critical milestone of enabling the finetuning of LLMs on phones and other low\n",
      "\n",
      "resource settings. While 7B models were shown to be able to be run on phones before, QLORAis\n",
      "\n",
      "the first method that would enable the finetuning of such models. We estimate that with an iPhone 12\n",
      "\n",
      "Plus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\n",
      "\n",
      "7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\n",
      "\n",
      "novel applications that have not been possible before due to privacy or LLM quality issues. QLORA\n",
      "\n",
      "can help enable privacy-preserving usage of LLMs, where users can own and manage their own data\n",
      "\n",
      "and models, while simultaneously making LLMs easier to deploy.\n",
      "\n",
      "However, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\n",
      "    answer the question base on the above context: what is qlora?\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='QL ORA: Efficient Finetuning of Quantized LLMs\\n\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\n\\nLuke Zettlemoyer\\n\\nUniversity of Washington\\n\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\n\\nAbstract\\n\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\n\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\n\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\n\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\n\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\n\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\n\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\n\\non a single GPU. QLORAintroduces a number of innovations to save memory\\n\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\n\\nis information theoretically optimal for normally distributed weights (b) Double', metadata={'source': '/Users/matansharon/python/chat_with_docs/data/text/qlora.txt', 'start_index': 0}), 0.6999662581526787), (Document(page_content='processing a mini-batch with a long sequence length. We combine these contributions into a better\\n\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\n\\nthe accuracy tradeoffs seen in prior work.\\n\\nQLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\n\\nperformance on model scales that would be impossible using regular finetuning due to memory\\n\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\n\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\n\\nrecovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also\\n\\nanalyze trends in the trained models. First, we find that data quality is far more important than\\n\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,', metadata={'source': '/Users/matansharon/python/chat_with_docs/data/text/qlora.txt', 'start_index': 5280}), 0.6889076837466932), (Document(page_content='large corporations and small teams with consumer GPUs.\\n\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\n\\nmight enable the critical milestone of enabling the finetuning of LLMs on phones and other low\\n\\nresource settings. While 7B models were shown to be able to be run on phones before, QLORAis\\n\\nthe first method that would enable the finetuning of such models. We estimate that with an iPhone 12\\n\\nPlus, QLORAcan finetune 3 million tokens per night while the phone is charging. While finetuned\\n\\n7B models do not reach the quality of ChatGPT, we believe that the quality is good enough to enable\\n\\nnovel applications that have not been possible before due to privacy or LLM quality issues. QLORA\\n\\ncan help enable privacy-preserving usage of LLMs, where users can own and manage their own data\\n\\nand models, while simultaneously making LLMs easier to deploy.\\n\\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of', metadata={'source': '/Users/matansharon/python/chat_with_docs/data/text/qlora.txt', 'start_index': 59464}), 0.6860621814163288)]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from helper_functions import *\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # st.header(\"Chat with me\")\n",
    "    chunks=load_and_split_documents()\n",
    "    db=create_db(chunks)\n",
    "    model=ChatOpenAI()\n",
    "    # st.write(\"I am ready to chat\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    query=\"what is qlora?\"\n",
    "    results=get_results_with_scores(query,db)\n",
    "    # st.write(results)\n",
    "    # st.write(db.similarity_search(query))\n",
    "    print(results)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
