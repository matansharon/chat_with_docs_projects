{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is Pixegami tutorial\n",
    "### from this video :https://www.youtube.com/watch?v=tcqEUSNCn8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chromadb.api.client.Client object at 0x10432fac0>\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import TextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "import json\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DIRECTORY_PATH='/Users/matansharon/python/chat_with_docs/AI_Apps/chat_with_txt/data'\n",
    "DOCS_PATH='/Users/matansharon/python/chat_with_docs/AI_Apps/chat_with_txt/docs.json'\n",
    "\n",
    "def load_and_read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_documents_list():\n",
    "    \n",
    "    with open(DOCS_PATH,'r') as f:\n",
    "        data=json.load(f)\n",
    "        docs=data['documents']\n",
    "    return docs\n",
    "\n",
    "def load_all_docs_in_data_folder():\n",
    "    loader = DirectoryLoader(DIRECTORY_PATH)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_text(text:str):\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks=text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def create_new_db(chunks):\n",
    "    chroma_client = chromadb.Client()\n",
    "    return chroma_client\n",
    "    # path='chroma_db'\n",
    "    # if not os.path.exists(path):\n",
    "        \n",
    "    #     db=Chroma.from_texts(texts=[''],embedding=OpenAIEmbeddings(),persist_directory=path)\n",
    "    #     return db\n",
    "    # return load_db()\n",
    "\n",
    "def load_db():\n",
    "    db = Chroma(persist_directory=\"chroma_db\",embedding_function=OpenAIEmbeddings())\n",
    "    return db\n",
    "\n",
    "def get_results_with_scores(query,db):\n",
    "    bar=0.5\n",
    "    res=db.similarity_search_with_relevance_scores(query,k=3)\n",
    "    \n",
    "    return res\n",
    "def get_prompt_template(results,query):\n",
    "    template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "    context_texts = []\n",
    "    for i in range(len(results)):\n",
    "        context_texts.append(results[i][0].page_content)\n",
    "    temp = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "    res=prompt_tamplate.format(context=temp,query=query)\n",
    "    return res\n",
    "\n",
    "def get_response(query,db,model):\n",
    "    results=get_results_with_scores(query,db)\n",
    "    prompt_template=get_prompt_template(results,query)\n",
    "    response=model.invoke(prompt_template)\n",
    "    return response.content\n",
    "\n",
    "def main_app():\n",
    "    db=create_new_db('')\n",
    "    print(db)\n",
    "main_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chunks)\n",
    "# db=Chroma.from_texts(texts=chunks,embedding=OpenAIEmbeddings())\n",
    "# model=ChatOpenAI()\n",
    "# query='what is qlora?'\n",
    "\n",
    "# response=get_response(query,db,model)\n",
    "# print(response)\n",
    "# db=create_new_db('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from PyPDF2 import PdfReader\n",
    "path='/Users/matansharon/python/chat_with_docs/data/pdf/qlora.pdf'\n",
    "doc=PdfReader(path)\n",
    "#get the name of the document\n",
    "doc_name=path.split('/')[-1]\n",
    "\n",
    "text=''\n",
    "for page in doc.pages:\n",
    "    text+=page.extract_text()\n",
    "\n",
    "\n",
    "\n",
    "chunks=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100,length_function=len,add_start_index=True).split_text(text)\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "                model_name=\"text-embedding-3-small\"\n",
    "            )\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection=chroma_client.create_collection(name=\"My_pdf_collection\",embedding_function= openai_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas=[{\"page_number\":i,\"page_content\":chunks[i],'doc_name':doc_name} for i in range(len(chunks))]\n",
    "emb_list = openai_ef(chunks)\n",
    "ids=[\"id\"+str(i) for i in range(len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(embeddings=emb_list,documents=chunks,metadatas=metadatas,ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id13', 'id0', 'id86']],\n",
       " 'distances': [[0.8332789540290833, 0.9855331182479858, 1.0096246004104614]],\n",
       " 'metadatas': [[{'doc_name': 'qlora.pdf',\n",
       "    'page_content': 'QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an',\n",
       "    'page_number': 13},\n",
       "   {'doc_name': 'qlora.pdf',\n",
       "    'page_content': 'QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double',\n",
       "    'page_number': 0},\n",
       "   {'doc_name': 'qlora.pdf',\n",
       "    'page_content': '21A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.',\n",
       "    'page_number': 86}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an',\n",
       "   'QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double',\n",
       "   '21A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=collection.query(query_texts=[\"what is qlora?\"],n_results=3)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8332789540290833\n",
      "0.9855331182479858\n",
      "1.0096246004104614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an',\n",
       " 'QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double',\n",
       " '21A QLoRA vs Standard Finetuning Experimental Setup Details\\nA.1 Hyperparameters for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_with_high_score=[]\n",
    "for i in range(len(res['ids'][0])):\n",
    "    if res['distances'][0][i]<1.1:\n",
    "        print(res['distances'][0][i])\n",
    "        results_with_high_score.append(res['documents'][0][i])\n",
    "results_with_high_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response('who is the authors of the paper \"Attention Is All You Need\"',db,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='QLORA is an efficient fine-tuning approach designed for quantized large language models (LLMs), which significantly reduces memory usage without sacrificing performance. The key innovations introduced by QLORA include:\\n\\n1. **4-bit NormalFloat (NF4) Quantization:** This is a new data type optimized for normally distributed weights, which is information-theoretically optimal. It helps in reducing the memory footprint of the model while maintaining the integrity of the data.\\n\\n2. **Double Quantization:** This technique further aids in memory optimization during the fine-tuning process.\\n\\n3. **Paged Optimizers:** To address the issue of memory spikes during gradient checkpointing, which can lead to out-of-memory errors on single machines, QLORA introduces Paged Optimizers. This innovation allows for efficient memory management, preventing such errors and making it feasible to fine-tune large models on a single machine.\\n\\n4. **Low-Precision Storage and High-Precision Computation:** QLORA employs a low-precision storage data type (typically 4-bit) for weight tensors and uses a higher precision data type (usually BFloat16) for computations. This means that when a weight tensor is used, it is dequantized to BFloat16 before performing matrix multiplication in 16-bit, striking a balance between memory efficiency and computational accuracy.\\n\\nQLORA stands out by allowing the fine-tuning of a 65 billion parameter model on a single 48GB GPU, preserving the full 16-bit fine-tuning task performance. The approach backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The Guanaco model family, developed using QLORA, has shown to outperform all previously openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT with only 24 hours of fine-tuning on a single GPU.\\n\\nAdditionally, QLORA involves a systematic hyperparameter search for LoRA, exploring variables such as LoRA dropout, LoRA r (rank), and LoRA layers, aiming to optimize the fine-tuning process across different model sizes and configurations.'\n"
     ]
    }
   ],
   "source": [
    "chat=ChatOpenAI(model='gpt-4-turbo-preview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"QLORA is an efficient finetuning approach designed to significantly reduce memory usage during the finetuning process of large language models (LLMs), enabling the finetuning of models with up to 65 billion parameters on a single 48GB GPU while preserving the full 16-bit finetuning task performance. It achieves this by backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The key innovations introduced by QLORA include:\\n\\n1. **4-bit NormalFloat (NF4) Quantization**: This is a new data type optimized for normally distributed weights. It builds upon Quantile Quantization, aiming to be information-theoretically optimal by ensuring each quantization bin has an equal number of data points, thus minimizing the loss of information due to quantization.\\n\\n2. **Double Quantization**: This technique is part of QLORA's approach to maintain high fidelity in the finetuning process, though the text does not provide an extensive explanation of its implementation or advantages.\\n\\n3. **Paged Optimizers**: To manage memory more efficiently and prevent out-of-memory errors during gradient checkpointing, which is crucial for allowing the finetuning of large models on a single machine.\\n\\nQLORA operates with a dual-data type approach for weights, using a low-precision (usually 4-bit) storage data type for weight tensors and a higher precision computation data type (usually BFloat16) for operations. This means that when a QLORA weight tensor is utilized, it is first dequantized to BFloat16 before any matrix multiplication or other operations are performed in 16-bit precision.\\n\\nThe introduction of these innovations allows QLORA to outperform all previously released models on the Vicuna benchmark, achieving 99.3% of the performance level of ChatGPT with only 24 hours of finetuning on a single GPU. This makes QLORA a significant step forward in the efficient and effective finetuning of quantized large language models.\"\n"
     ]
    }
   ],
   "source": [
    "print(chat.invoke(f'base on this: {content} what is qlora?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2=Chroma.from_documents(documents=chunks,embedding=OpenAIEmbeddings(),persist_directory='chorma_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response('what is qlora?',db2,chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the json file\n",
    "import json\n",
    "with open('docs.json') as f:\n",
    "    data = json.load(f)\n",
    "    print(data['documents'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
