{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is Pixegami tutorial\n",
    "### from this video :https://www.youtube.com/watch?v=tcqEUSNCn8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splited_docs 101\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import TextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datetime import datetime\n",
    "import json\n",
    "import PyPDF2\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DIRECTORY_PATH='/Users/matansharon/python/chat_with_docs/AI_Apps/chat_with_txt/data'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_documents_names():\n",
    "    documents_names = os.listdir(DIRECTORY_PATH)\n",
    "    return documents_names\n",
    "def load_all_docs_in_data_folder(documents_names):\n",
    "    data_documents= []\n",
    "    for doc in documents_names:\n",
    "        path=os.path.join(DIRECTORY_PATH+'/',doc)\n",
    "        Document=PyPDF2.PdfReader(path)\n",
    "        text = ''\n",
    "        for page in Document.pages:\n",
    "            text += page.extract_text()\n",
    "        data_documents.append(text)\n",
    "        \n",
    "    return data_documents\n",
    "    \n",
    "    \n",
    "\n",
    "def split_text(text:str):\n",
    "    \n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks=text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def create_new_db(chunks):\n",
    "    \n",
    "    path='chroma_db'\n",
    "    if chunks:\n",
    "        print('here')\n",
    "        db=Chroma.from_texts(texts=chunks,embedding=OpenAIEmbeddings(model='text-embedding-3-small'),persist_directory=path)\n",
    "        return db\n",
    "    if not os.path.exists(path):\n",
    "        \n",
    "        db=Chroma.from_texts(texts=[''],embedding=OpenAIEmbeddings(model='text-embedding-3-small'),persist_directory=path)\n",
    "        return db\n",
    "    return load_db()\n",
    "\n",
    "def load_db():\n",
    "    db = Chroma(persist_directory=\"chroma_db\",embedding_function=OpenAIEmbeddings(model='text-embedding-3-small'))\n",
    "    return db\n",
    "\n",
    "def get_results_with_scores(query,db):\n",
    "    \n",
    "    res=db.similarity_search_with_relevance_scores(query,k=3)\n",
    "    \n",
    "    return res\n",
    "def get_prompt_template(results,query):\n",
    "    template=\"\"\"\n",
    "    answer the question base only on the following context:\n",
    "    {context}\n",
    "    answer the question base on the above context: {query}\n",
    "    \n",
    "    \"\"\"\n",
    "    context_texts = []\n",
    "    for i in range(len(results)):\n",
    "        context_texts.append(results[i][0].page_content)\n",
    "    temp = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "    prompt_tamplate=ChatPromptTemplate.from_template(template)\n",
    "    res=prompt_tamplate.format(context=temp,query=query)\n",
    "    return res\n",
    "\n",
    "def get_response(query,db,model):\n",
    "    results=get_results_with_scores(query,db)\n",
    "    prompt_template=get_prompt_template(results,query)\n",
    "    response=model.invoke(prompt_template)\n",
    "    return response.content\n",
    "\n",
    "def main_app():\n",
    "\n",
    "    documents_names=get_documents_names()\n",
    "    docs=load_all_docs_in_data_folder(documents_names=documents_names)\n",
    "    splited_docs=[]\n",
    "    for doc in docs:\n",
    "        chunks=split_text(doc)\n",
    "        for chunk in chunks:\n",
    "            splited_docs.append(chunk)\n",
    "            \n",
    "    print('splited_docs',len(splited_docs))\n",
    "    db=create_new_db(splited_docs)\n",
    "    model=ChatOpenAI()\n",
    "    \n",
    "    return db,model\n",
    "db,model=main_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Our QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod'),\n",
       "  0.21952786243575118),\n",
       " (Document(page_content='fully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have'),\n",
       "  0.20652446385207912),\n",
       " (Document(page_content='finetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the\\ninstructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\\nlearning rate for 33B and 65B while doubling the batch size.'),\n",
       "  0.2044900354744582)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results_with_scores('what is Finetuning?',db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\n    answer the question base only on the following context:\\n    Our QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources, a big win for the accessibility of state of the art NLP\\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones. We believe our QLORAmethod\\n\\n---\\n\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have\\n\\n---\\n\\nfinetuning with cross-entropy loss (supervised learning) without reinforcement learning, even for\\ndatasets that include human judgments of different responses. For datasets that have a clear distinction\\nbetween instruction and response, we finetune only on the response (see ablations in Appendix B).\\nFor OASST1 and HH-RLHF, multiple responses are available. We then select the top response at\\nevery level of the conversation tree and finetune on the full selected conversation, including the\\ninstructions. In all of our experiments, we use NF4 QLORAwith double quantization and paged\\noptimizers to prevent memory spikes during gradient checkpointing. We do small hyperparameter\\nsearches for the 13B and 33B LLaMA models and we find that all hyperparameter settings found\\nat 7B generalize (including number of epochs) except learning rate and batch size. We halve the\\nlearning rate for 33B and 65B while doubling the batch size.\\n    answer the question base on the above context: what is Finetuning?\\n    \\n    '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp=get_prompt_template(get_results_with_scores('what is Finetuning?',db),'what is Finetuning?')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Finetuning is the process of adjusting and optimizing a pre-trained language model by further training it on a specific dataset or task, in order to improve its performance on that particular task. In the context provided, finetuning is used to transform raw pretrained large language models (LLMs) into chatbot models like ChatGPT, with the goal of achieving state-of-the-art natural language processing (NLP) technology.')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There is no widely known or established meaning for the term \"qlora.\" It is possible that it could be a unique or made-up word with no specific definition. If you have more context or information about the term, please provide it so that a more accurate answer can be given.')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=ChatOpenAI()\n",
    "model2.invoke(\"what is qlora?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLORA is a technique that achieves high-fidelity 4-bit finetuning using 4-bit NormalFloat (NF4) quantization and Double Quantization. It also introduces Paged Optimizers to prevent memory spikes during gradient checkpointing. It has one low-precision storage data type (4-bit) and one computation data type (BFloat16). When using a QLORA weight tensor, the tensor is dequantized to BFloat16 and matrix multiplication is performed in 16-bit.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"what is qlora?\",db,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chunks)\n",
    "\n",
    "# model=ChatOpenAI()\n",
    "# query='what is qlora?'\n",
    "\n",
    "# response=get_response(query,db,model)\n",
    "# print(response)\n",
    "# db=create_new_db('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from PyPDF2 import PdfReader\n",
    "path='/Users/matansharon/python/chat_with_docs/data/pdf/qlora.pdf'\n",
    "doc=PdfReader(path)\n",
    "#get the name of the document\n",
    "doc_name=path.split('/')[-1]\n",
    "\n",
    "text=''\n",
    "for page in doc.pages:\n",
    "    text+=page.extract_text()\n",
    "\n",
    "\n",
    "\n",
    "chunks=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100,length_function=len,add_start_index=True).split_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response('who is the authors of the paper \"Attention Is All You Need\"',db,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat=ChatOpenAI(model='gpt-4-turbo-preview')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat.invoke(f'base on this: {content} what is qlora?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
